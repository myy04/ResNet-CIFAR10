{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myy04/ResNet-CIFAR10/blob/main/hw2_2_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOdYjN30cxPp"
      },
      "source": [
        "# ELEC4542 - Homework 2\n",
        "\n",
        "In this notebook, we will walk through how to build and train a CNN to for image classification.\n",
        "\n",
        "We will use PyTorch and torchvision to build and train our model.\n",
        "\n",
        "Now, take a deep breath, prepare yourself, as we are about to get your hands dirty!\n",
        "\n",
        "**You are expected to complete every code blocks marked with `TODO`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qQ-VDsg431gC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import optim, nn, Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from __future__ import annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLdsrSXq31gD"
      },
      "source": [
        "## Part 2: CIFAR-10 image classification\n",
        "\n",
        "Nicely done!\n",
        "\n",
        "We have built a simple CNN model and trained it on MNIST dataset.\n",
        "\n",
        "Now, let's move on to a more challenging dataset - CIFAR-10.\n",
        "\n",
        "CIFAR-10 is a dataset of 32x32 color images of 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).\n",
        "It has 50,000 training images and 10,000 test images.\n",
        "\n",
        "In this part, we will be using an advanced CNN model (ResNet-18) taught in the lecture to classify CIFAR-10 images.\n",
        "\n",
        "Try your best to achieve the best performance on CIFAR-10 dataset!\n",
        "Note that your mark will NOT based on the performance of your model.\n",
        "Instead, it will be based on the training process.\n",
        "\n",
        "Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec9DnIk-31gE",
        "outputId": "cd9f09c0-7162-4cb4-e9ee-9bec760d7197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform = v2.Compose(\n",
        "    [\n",
        "        #############################################################################\n",
        "        # TODO:                                                                     #\n",
        "        # 1. update mean and std given the statistics of training data              #\n",
        "        # 2. set your own data augmentation combinations                            #\n",
        "        #############################################################################\n",
        "\n",
        "        v2.RandomCrop(32, padding = 4, padding_mode = \"reflect\"),\n",
        "        v2.RandomHorizontalFlip(p = 0.5),\n",
        "        v2.ColorJitter(brightness = 0.1,contrast = 0.1,saturation = 0.1),\n",
        "        v2.RandomAdjustSharpness(sharpness_factor = 2,p = 0.2),\n",
        "\n",
        "\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize(mean, std),\n",
        "        #############################################################################\n",
        "        #                          END OF YOUR CODE                                 #\n",
        "        #############################################################################\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\"data\", transform=transform, train=True, download=True)\n",
        "test_dataset = datasets.CIFAR10(\"data\", transform=transform, train=False, download=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "#############################################################################\n",
        "# TODO:                                                                     #\n",
        "# 1. Use a pre-defined model in torchvision.models                          #\n",
        "# 2. Define the criterion                                                   #\n",
        "# 3. Define the optimizer                                                   #\n",
        "# 4. Adjust the hyperparameters                                             #\n",
        "#############################################################################\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained = False)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-3, momentum=0.7, weight_decay=5e-7)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 20, eta_min=1e-5)\n",
        "\n",
        "\n",
        "#############################################################################\n",
        "#                          END OF YOUR CODE                                 #\n",
        "#############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JcQ5ga-a31gE"
      },
      "outputs": [],
      "source": [
        "def train_step(input: Tensor, label: Tensor):\n",
        "    #############################################################################\n",
        "    # TODO: implement a training step                                           #\n",
        "    #############################################################################\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    input = input.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    output = model(input)\n",
        "    loss = criterion(output, label)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    # scheduler.step()\n",
        "\n",
        "    return loss, output\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_step(input: Tensor, label: Tensor):\n",
        "    #############################################################################\n",
        "    # TODO: implement a testing step                                           #\n",
        "    #############################################################################\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    input = input.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    output = model(input)\n",
        "    loss = criterion(output, label)\n",
        "\n",
        "    return loss, output\n",
        "\n",
        "\n",
        "def train_epoch(dataloader):\n",
        "    correct = 0\n",
        "    samples = 0\n",
        "\n",
        "    for i, (input, label) in enumerate(dataloader):\n",
        "        loss, output = train_step(input, label)\n",
        "\n",
        "        #############################################################################\n",
        "        # TODO: implement accuracy calculation                                      #\n",
        "        # You may add additional metrics to better evaluate your model              #\n",
        "        #############################################################################\n",
        "\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "        samples += label.size(0)\n",
        "        correct += (predicted == label.to(device)).sum().item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Loss [{i}/{len(dataloader)}]: {loss.item()}\")\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Train Accuracy: {correct / samples * 100.00}%\")\n",
        "\n",
        "        #############################################################################\n",
        "        #                          END OF YOUR CODE                                 #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "def test_epoch(dataloader):\n",
        "    correct = 0\n",
        "    samples = 0\n",
        "\n",
        "    for i, (input, label) in enumerate(dataloader):\n",
        "        loss, output = test_step(input, label)\n",
        "        #############################################################################\n",
        "        # TODO: implement accuracy calculation                                      #\n",
        "        # You may add additional metrics to better evaluate your model              #\n",
        "        #############################################################################\n",
        "\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "        samples += label.size(0)\n",
        "        correct += (predicted == label.to(device)).sum().item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Loss [{i}/{len(dataloader)}]: {loss.item()}\")\n",
        "\n",
        "    print(f\"Test Accuracy: {correct / samples * 100.00}%\")\n",
        "\n",
        "        #############################################################################\n",
        "        #                          END OF YOUR CODE                                 #\n",
        "        #############################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psrDjoe231gE",
        "outputId": "ca1e3cd6-45af-449b-e944-f9f579858e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "Loss [0/6250]: 2.9934277534484863\n",
            "Loss [100/6250]: 3.089221477508545\n",
            "Loss [200/6250]: 2.036905527114868\n",
            "Loss [300/6250]: 2.485729694366455\n",
            "Loss [400/6250]: 2.6440584659576416\n",
            "Loss [500/6250]: 2.4041454792022705\n",
            "Loss [600/6250]: 2.0921390056610107\n",
            "Loss [700/6250]: 2.1698484420776367\n",
            "Loss [800/6250]: 1.545264720916748\n",
            "Loss [900/6250]: 2.3604073524475098\n",
            "Loss [1000/6250]: 1.9104199409484863\n",
            "Loss [1100/6250]: 1.5051521062850952\n",
            "Loss [1200/6250]: 2.085587501525879\n",
            "Loss [1300/6250]: 1.6903482675552368\n",
            "Loss [1400/6250]: 1.967579960823059\n",
            "Loss [1500/6250]: 2.110064744949341\n",
            "Loss [1600/6250]: 1.7403547763824463\n",
            "Loss [1700/6250]: 2.2502613067626953\n",
            "Loss [1800/6250]: 2.087735652923584\n",
            "Loss [1900/6250]: 1.560302734375\n",
            "Loss [2000/6250]: 1.9601447582244873\n",
            "Loss [2100/6250]: 2.2211320400238037\n",
            "Loss [2200/6250]: 1.3972539901733398\n",
            "Loss [2300/6250]: 1.4767810106277466\n",
            "Loss [2400/6250]: 2.3401336669921875\n",
            "Loss [2500/6250]: 2.0098178386688232\n",
            "Loss [2600/6250]: 1.6433379650115967\n",
            "Loss [2700/6250]: 1.6231000423431396\n",
            "Loss [2800/6250]: 2.4524717330932617\n",
            "Loss [2900/6250]: 1.459427833557129\n",
            "Loss [3000/6250]: 1.745713710784912\n",
            "Loss [3100/6250]: 2.1365106105804443\n",
            "Loss [3200/6250]: 1.449642539024353\n",
            "Loss [3300/6250]: 2.0266220569610596\n",
            "Loss [3400/6250]: 1.7382738590240479\n",
            "Loss [3500/6250]: 2.2246925830841064\n",
            "Loss [3600/6250]: 1.4829245805740356\n",
            "Loss [3700/6250]: 1.3216307163238525\n",
            "Loss [3800/6250]: 1.380682110786438\n",
            "Loss [3900/6250]: 1.7561947107315063\n",
            "Loss [4000/6250]: 1.748120665550232\n",
            "Loss [4100/6250]: 1.5055687427520752\n",
            "Loss [4200/6250]: 1.7123668193817139\n",
            "Loss [4300/6250]: 0.8838676810264587\n",
            "Loss [4400/6250]: 1.6225800514221191\n",
            "Loss [4500/6250]: 1.5058706998825073\n",
            "Loss [4600/6250]: 1.7609131336212158\n",
            "Loss [4700/6250]: 1.8803154230117798\n",
            "Loss [4800/6250]: 2.1272177696228027\n",
            "Loss [4900/6250]: 2.6563456058502197\n",
            "Loss [5000/6250]: 1.4920862913131714\n",
            "Loss [5100/6250]: 1.2512085437774658\n",
            "Loss [5200/6250]: 1.6091862916946411\n",
            "Loss [5300/6250]: 1.8020045757293701\n",
            "Loss [5400/6250]: 1.4178249835968018\n",
            "Loss [5500/6250]: 1.3628956079483032\n",
            "Loss [5600/6250]: 1.50503671169281\n",
            "Loss [5700/6250]: 1.6143900156021118\n",
            "Loss [5800/6250]: 1.5318948030471802\n",
            "Loss [5900/6250]: 1.5842154026031494\n",
            "Loss [6000/6250]: 1.7487263679504395\n",
            "Loss [6100/6250]: 1.5903633832931519\n",
            "Loss [6200/6250]: 1.3616430759429932\n",
            "Train Accuracy: 33.744%\n",
            "Loss [0/1250]: 1.1550267934799194\n",
            "Loss [100/1250]: 1.0747917890548706\n",
            "Loss [200/1250]: 1.8064544200897217\n",
            "Loss [300/1250]: 1.1677632331848145\n",
            "Loss [400/1250]: 1.8970484733581543\n",
            "Loss [500/1250]: 2.1061689853668213\n",
            "Loss [600/1250]: 1.343076467514038\n",
            "Loss [700/1250]: 1.736959457397461\n",
            "Loss [800/1250]: 1.6014628410339355\n",
            "Loss [900/1250]: 1.6304954290390015\n",
            "Loss [1000/1250]: 1.7592356204986572\n",
            "Loss [1100/1250]: 1.3521599769592285\n",
            "Loss [1200/1250]: 1.7775297164916992\n",
            "Test Accuracy: 46.150000000000006%\n",
            "epoch: 1\n",
            "Loss [0/6250]: 2.16491436958313\n",
            "Loss [100/6250]: 2.182067394256592\n",
            "Loss [200/6250]: 1.6226359605789185\n",
            "Loss [300/6250]: 1.8842523097991943\n",
            "Loss [400/6250]: 1.9693530797958374\n",
            "Loss [500/6250]: 1.1703540086746216\n",
            "Loss [600/6250]: 1.8132119178771973\n",
            "Loss [700/6250]: 1.7981579303741455\n",
            "Loss [800/6250]: 1.5504090785980225\n",
            "Loss [900/6250]: 1.9832124710083008\n",
            "Loss [1000/6250]: 1.758147954940796\n",
            "Loss [1100/6250]: 1.420189619064331\n",
            "Loss [1200/6250]: 0.9242227077484131\n",
            "Loss [1300/6250]: 1.1965370178222656\n",
            "Loss [1400/6250]: 1.226883888244629\n",
            "Loss [1500/6250]: 1.1296467781066895\n",
            "Loss [1600/6250]: 2.319366455078125\n",
            "Loss [1700/6250]: 1.5464773178100586\n",
            "Loss [1800/6250]: 1.1063628196716309\n",
            "Loss [1900/6250]: 1.3029495477676392\n",
            "Loss [2000/6250]: 1.3167595863342285\n",
            "Loss [2100/6250]: 1.3063145875930786\n",
            "Loss [2200/6250]: 1.4248988628387451\n",
            "Loss [2300/6250]: 1.1359905004501343\n",
            "Loss [2400/6250]: 2.8397209644317627\n",
            "Loss [2500/6250]: 1.2974308729171753\n",
            "Loss [2600/6250]: 1.9069433212280273\n",
            "Loss [2700/6250]: 1.79998779296875\n",
            "Loss [2800/6250]: 1.467836856842041\n",
            "Loss [2900/6250]: 2.0244476795196533\n",
            "Loss [3000/6250]: 1.658858060836792\n",
            "Loss [3100/6250]: 1.624943494796753\n",
            "Loss [3200/6250]: 1.1348118782043457\n",
            "Loss [3300/6250]: 1.4920763969421387\n",
            "Loss [3400/6250]: 1.1278340816497803\n",
            "Loss [3500/6250]: 1.1863995790481567\n",
            "Loss [3600/6250]: 1.5093193054199219\n",
            "Loss [3700/6250]: 1.717278242111206\n",
            "Loss [3800/6250]: 1.5939172506332397\n",
            "Loss [3900/6250]: 1.395024299621582\n",
            "Loss [4000/6250]: 1.0437220335006714\n",
            "Loss [4100/6250]: 1.5777721405029297\n",
            "Loss [4200/6250]: 1.3149389028549194\n",
            "Loss [4300/6250]: 1.1732475757598877\n",
            "Loss [4400/6250]: 1.8107985258102417\n",
            "Loss [4500/6250]: 1.7766296863555908\n",
            "Loss [4600/6250]: 1.2473254203796387\n",
            "Loss [4700/6250]: 1.3454965353012085\n",
            "Loss [4800/6250]: 1.4086322784423828\n",
            "Loss [4900/6250]: 1.0643876791000366\n",
            "Loss [5000/6250]: 1.5999360084533691\n",
            "Loss [5100/6250]: 1.3672723770141602\n",
            "Loss [5200/6250]: 1.0667681694030762\n",
            "Loss [5300/6250]: 0.9594465494155884\n",
            "Loss [5400/6250]: 2.0770628452301025\n",
            "Loss [5500/6250]: 1.3207190036773682\n",
            "Loss [5600/6250]: 1.7223631143569946\n",
            "Loss [5700/6250]: 2.4290611743927\n",
            "Loss [5800/6250]: 1.4041069746017456\n",
            "Loss [5900/6250]: 2.0091171264648438\n",
            "Loss [6000/6250]: 1.2044823169708252\n",
            "Loss [6100/6250]: 1.2728321552276611\n",
            "Loss [6200/6250]: 2.093019962310791\n",
            "Train Accuracy: 44.738%\n",
            "Loss [0/1250]: 0.9133694171905518\n",
            "Loss [100/1250]: 1.159265160560608\n",
            "Loss [200/1250]: 1.6523878574371338\n",
            "Loss [300/1250]: 1.393823504447937\n",
            "Loss [400/1250]: 1.579393744468689\n",
            "Loss [500/1250]: 1.6280133724212646\n",
            "Loss [600/1250]: 0.9827591776847839\n",
            "Loss [700/1250]: 1.6821300983428955\n",
            "Loss [800/1250]: 1.2500145435333252\n",
            "Loss [900/1250]: 1.7522649765014648\n",
            "Loss [1000/1250]: 1.9116814136505127\n",
            "Loss [1100/1250]: 1.0953348875045776\n",
            "Loss [1200/1250]: 1.5556292533874512\n",
            "Test Accuracy: 53.05%\n",
            "epoch: 2\n",
            "Loss [0/6250]: 1.022169589996338\n",
            "Loss [100/6250]: 1.8360438346862793\n",
            "Loss [200/6250]: 2.199528455734253\n",
            "Loss [300/6250]: 1.0505138635635376\n",
            "Loss [400/6250]: 1.1960117816925049\n",
            "Loss [500/6250]: 1.2934399843215942\n",
            "Loss [600/6250]: 1.0199899673461914\n",
            "Loss [700/6250]: 1.942833423614502\n",
            "Loss [800/6250]: 1.757623553276062\n",
            "Loss [900/6250]: 1.6226776838302612\n",
            "Loss [1000/6250]: 1.2594382762908936\n",
            "Loss [1100/6250]: 2.3291478157043457\n",
            "Loss [1200/6250]: 1.037131905555725\n",
            "Loss [1300/6250]: 2.3024916648864746\n",
            "Loss [1400/6250]: 2.0480642318725586\n",
            "Loss [1500/6250]: 1.170924186706543\n",
            "Loss [1600/6250]: 1.5225677490234375\n",
            "Loss [1700/6250]: 1.7650810480117798\n",
            "Loss [1800/6250]: 1.2842551469802856\n",
            "Loss [1900/6250]: 2.321892261505127\n",
            "Loss [2000/6250]: 2.1934421062469482\n",
            "Loss [2100/6250]: 1.0252385139465332\n",
            "Loss [2200/6250]: 0.8024333119392395\n",
            "Loss [2300/6250]: 0.9913397431373596\n",
            "Loss [2400/6250]: 1.3886750936508179\n",
            "Loss [2500/6250]: 1.3974568843841553\n",
            "Loss [2600/6250]: 1.8785631656646729\n",
            "Loss [2700/6250]: 1.1024179458618164\n",
            "Loss [2800/6250]: 1.6405103206634521\n",
            "Loss [2900/6250]: 1.1418403387069702\n",
            "Loss [3000/6250]: 0.8871452212333679\n",
            "Loss [3100/6250]: 1.5890426635742188\n",
            "Loss [3200/6250]: 1.5231870412826538\n",
            "Loss [3300/6250]: 1.7735505104064941\n",
            "Loss [3400/6250]: 0.9319600462913513\n",
            "Loss [3500/6250]: 1.8436856269836426\n",
            "Loss [3600/6250]: 1.3630813360214233\n",
            "Loss [3700/6250]: 2.7110705375671387\n",
            "Loss [3800/6250]: 1.1251156330108643\n",
            "Loss [3900/6250]: 0.6991574168205261\n",
            "Loss [4000/6250]: 1.3637604713439941\n",
            "Loss [4100/6250]: 1.2528085708618164\n",
            "Loss [4200/6250]: 1.3777744770050049\n",
            "Loss [4300/6250]: 0.9500344395637512\n",
            "Loss [4400/6250]: 1.7500652074813843\n",
            "Loss [4500/6250]: 0.9118592739105225\n",
            "Loss [4600/6250]: 1.472234845161438\n",
            "Loss [4700/6250]: 0.9848085045814514\n",
            "Loss [4800/6250]: 1.5539871454238892\n",
            "Loss [4900/6250]: 0.6945018768310547\n",
            "Loss [5000/6250]: 0.9007173776626587\n",
            "Loss [5100/6250]: 1.827649474143982\n",
            "Loss [5200/6250]: 1.2941668033599854\n",
            "Loss [5300/6250]: 1.4898245334625244\n",
            "Loss [5400/6250]: 1.174565315246582\n",
            "Loss [5500/6250]: 1.5054075717926025\n",
            "Loss [5600/6250]: 1.2124874591827393\n",
            "Loss [5700/6250]: 1.0589241981506348\n",
            "Loss [5800/6250]: 1.4258393049240112\n",
            "Loss [5900/6250]: 1.789139747619629\n",
            "Loss [6000/6250]: 0.9960283041000366\n",
            "Loss [6100/6250]: 1.0784752368927002\n",
            "Loss [6200/6250]: 2.1244957447052\n",
            "Train Accuracy: 50.3%\n",
            "Loss [0/1250]: 1.0727577209472656\n",
            "Loss [100/1250]: 1.383832335472107\n",
            "Loss [200/1250]: 1.41262686252594\n",
            "Loss [300/1250]: 1.0452247858047485\n",
            "Loss [400/1250]: 1.4541857242584229\n",
            "Loss [500/1250]: 1.7882390022277832\n",
            "Loss [600/1250]: 1.1594176292419434\n",
            "Loss [700/1250]: 1.511124610900879\n",
            "Loss [800/1250]: 1.239748239517212\n",
            "Loss [900/1250]: 1.2565572261810303\n",
            "Loss [1000/1250]: 1.8843096494674683\n",
            "Loss [1100/1250]: 0.9704259037971497\n",
            "Loss [1200/1250]: 1.8466598987579346\n",
            "Test Accuracy: 56.2%\n",
            "epoch: 3\n",
            "Loss [0/6250]: 1.3137602806091309\n",
            "Loss [100/6250]: 1.660982608795166\n",
            "Loss [200/6250]: 1.0373972654342651\n",
            "Loss [300/6250]: 1.6338121891021729\n",
            "Loss [400/6250]: 0.6855068206787109\n",
            "Loss [500/6250]: 1.0334974527359009\n",
            "Loss [600/6250]: 0.6742067337036133\n",
            "Loss [700/6250]: 1.1841025352478027\n",
            "Loss [800/6250]: 1.0230319499969482\n",
            "Loss [900/6250]: 0.7363052368164062\n",
            "Loss [1000/6250]: 1.8942981958389282\n",
            "Loss [1100/6250]: 1.010935664176941\n",
            "Loss [1200/6250]: 1.7562286853790283\n",
            "Loss [1300/6250]: 1.528042197227478\n",
            "Loss [1400/6250]: 1.1259043216705322\n",
            "Loss [1500/6250]: 0.9494472146034241\n",
            "Loss [1600/6250]: 2.0238561630249023\n",
            "Loss [1700/6250]: 1.114572525024414\n",
            "Loss [1800/6250]: 0.9776423573493958\n",
            "Loss [1900/6250]: 1.4558426141738892\n",
            "Loss [2000/6250]: 1.8023686408996582\n",
            "Loss [2100/6250]: 1.9074501991271973\n",
            "Loss [2200/6250]: 1.3167457580566406\n",
            "Loss [2300/6250]: 1.836974024772644\n",
            "Loss [2400/6250]: 1.549748182296753\n",
            "Loss [2500/6250]: 0.8882848024368286\n",
            "Loss [2600/6250]: 1.1952568292617798\n",
            "Loss [2700/6250]: 2.896249532699585\n",
            "Loss [2800/6250]: 1.3255975246429443\n",
            "Loss [2900/6250]: 1.3641244173049927\n",
            "Loss [3000/6250]: 1.6230483055114746\n",
            "Loss [3100/6250]: 1.0961285829544067\n",
            "Loss [3200/6250]: 0.6547273397445679\n",
            "Loss [3300/6250]: 1.090850591659546\n",
            "Loss [3400/6250]: 0.769386351108551\n",
            "Loss [3500/6250]: 1.2045400142669678\n",
            "Loss [3600/6250]: 1.6350984573364258\n",
            "Loss [3700/6250]: 1.5361746549606323\n",
            "Loss [3800/6250]: 1.3555536270141602\n",
            "Loss [3900/6250]: 1.2920588254928589\n",
            "Loss [4000/6250]: 1.2092540264129639\n",
            "Loss [4100/6250]: 0.7011175751686096\n",
            "Loss [4200/6250]: 1.537733793258667\n",
            "Loss [4300/6250]: 1.5012035369873047\n",
            "Loss [4400/6250]: 1.0808022022247314\n",
            "Loss [4500/6250]: 1.1174427270889282\n",
            "Loss [4600/6250]: 1.1263930797576904\n",
            "Loss [4700/6250]: 2.142529010772705\n",
            "Loss [4800/6250]: 1.4654814004898071\n",
            "Loss [4900/6250]: 1.1156002283096313\n",
            "Loss [5000/6250]: 0.92099928855896\n",
            "Loss [5100/6250]: 1.6435335874557495\n",
            "Loss [5200/6250]: 0.7092505097389221\n",
            "Loss [5300/6250]: 1.3234362602233887\n",
            "Loss [5400/6250]: 0.9124208092689514\n",
            "Loss [5500/6250]: 1.5347235202789307\n",
            "Loss [5600/6250]: 1.3837618827819824\n",
            "Loss [5700/6250]: 0.8064495325088501\n",
            "Loss [5800/6250]: 0.9547470211982727\n",
            "Loss [5900/6250]: 2.659492015838623\n",
            "Loss [6000/6250]: 0.689326286315918\n",
            "Loss [6100/6250]: 1.587700366973877\n",
            "Loss [6200/6250]: 0.7568925023078918\n",
            "Train Accuracy: 54.44%\n",
            "Loss [0/1250]: 0.8141306638717651\n",
            "Loss [100/1250]: 1.5395997762680054\n",
            "Loss [200/1250]: 1.2393141984939575\n",
            "Loss [300/1250]: 1.7704503536224365\n",
            "Loss [400/1250]: 1.7153440713882446\n",
            "Loss [500/1250]: 1.8114968538284302\n",
            "Loss [600/1250]: 0.9305658936500549\n",
            "Loss [700/1250]: 1.1857008934020996\n",
            "Loss [800/1250]: 2.070667028427124\n",
            "Loss [900/1250]: 1.5251402854919434\n",
            "Loss [1000/1250]: 1.61025869846344\n",
            "Loss [1100/1250]: 0.9246233701705933\n",
            "Loss [1200/1250]: 1.6487592458724976\n",
            "Test Accuracy: 57.99999999999999%\n",
            "epoch: 4\n",
            "Loss [0/6250]: 0.9456116557121277\n",
            "Loss [100/6250]: 1.8672428131103516\n",
            "Loss [200/6250]: 1.4227555990219116\n",
            "Loss [300/6250]: 0.6972864270210266\n",
            "Loss [400/6250]: 1.2362143993377686\n",
            "Loss [500/6250]: 1.9005712270736694\n",
            "Loss [600/6250]: 1.144430160522461\n",
            "Loss [700/6250]: 1.3943290710449219\n",
            "Loss [800/6250]: 1.3149540424346924\n",
            "Loss [900/6250]: 1.6020911931991577\n",
            "Loss [1000/6250]: 0.9103258848190308\n",
            "Loss [1100/6250]: 2.0140819549560547\n",
            "Loss [1200/6250]: 0.6822639107704163\n",
            "Loss [1300/6250]: 1.954685926437378\n",
            "Loss [1400/6250]: 1.1296961307525635\n",
            "Loss [1500/6250]: 0.6569039225578308\n",
            "Loss [1600/6250]: 2.008449077606201\n",
            "Loss [1700/6250]: 1.0103269815444946\n",
            "Loss [1800/6250]: 1.1986902952194214\n",
            "Loss [1900/6250]: 0.6288448572158813\n",
            "Loss [2000/6250]: 1.6477769613265991\n",
            "Loss [2100/6250]: 1.5914809703826904\n",
            "Loss [2200/6250]: 1.3811508417129517\n",
            "Loss [2300/6250]: 1.2044169902801514\n",
            "Loss [2400/6250]: 1.1709736585617065\n",
            "Loss [2500/6250]: 1.3597455024719238\n",
            "Loss [2600/6250]: 1.5883173942565918\n",
            "Loss [2700/6250]: 0.8029319643974304\n",
            "Loss [2800/6250]: 2.135173797607422\n",
            "Loss [2900/6250]: 1.0076183080673218\n",
            "Loss [3000/6250]: 0.777497410774231\n",
            "Loss [3100/6250]: 0.7375044226646423\n",
            "Loss [3200/6250]: 1.0564720630645752\n",
            "Loss [3300/6250]: 1.7049944400787354\n",
            "Loss [3400/6250]: 1.1984531879425049\n",
            "Loss [3500/6250]: 1.7482175827026367\n",
            "Loss [3600/6250]: 1.2442517280578613\n",
            "Loss [3700/6250]: 0.9739106297492981\n",
            "Loss [3800/6250]: 0.6037469506263733\n",
            "Loss [3900/6250]: 1.459467887878418\n",
            "Loss [4000/6250]: 0.534413754940033\n",
            "Loss [4100/6250]: 1.0072239637374878\n",
            "Loss [4200/6250]: 0.7265516519546509\n",
            "Loss [4300/6250]: 1.0904158353805542\n",
            "Loss [4400/6250]: 0.49495765566825867\n",
            "Loss [4500/6250]: 1.094559907913208\n",
            "Loss [4600/6250]: 2.12343168258667\n",
            "Loss [4700/6250]: 1.381651759147644\n",
            "Loss [4800/6250]: 0.996361494064331\n",
            "Loss [4900/6250]: 2.117262840270996\n",
            "Loss [5000/6250]: 1.1156312227249146\n",
            "Loss [5100/6250]: 1.1292322874069214\n",
            "Loss [5200/6250]: 0.5023423433303833\n",
            "Loss [5300/6250]: 1.109803318977356\n",
            "Loss [5400/6250]: 2.1791179180145264\n",
            "Loss [5500/6250]: 1.8369957208633423\n",
            "Loss [5600/6250]: 1.4325637817382812\n",
            "Loss [5700/6250]: 0.9699954986572266\n",
            "Loss [5800/6250]: 0.7799801826477051\n",
            "Loss [5900/6250]: 0.9496226906776428\n",
            "Loss [6000/6250]: 1.2500677108764648\n",
            "Loss [6100/6250]: 1.8872078657150269\n",
            "Loss [6200/6250]: 1.4426498413085938\n",
            "Train Accuracy: 57.728%\n",
            "Loss [0/1250]: 0.7754166722297668\n",
            "Loss [100/1250]: 0.9730784893035889\n",
            "Loss [200/1250]: 1.6118292808532715\n",
            "Loss [300/1250]: 2.205319881439209\n",
            "Loss [400/1250]: 1.0792640447616577\n",
            "Loss [500/1250]: 1.3511123657226562\n",
            "Loss [600/1250]: 1.026289939880371\n",
            "Loss [700/1250]: 1.442960262298584\n",
            "Loss [800/1250]: 0.9001808166503906\n",
            "Loss [900/1250]: 1.6692185401916504\n",
            "Loss [1000/1250]: 1.0565205812454224\n",
            "Loss [1100/1250]: 0.814924418926239\n",
            "Loss [1200/1250]: 1.0536587238311768\n",
            "Test Accuracy: 63.3%\n",
            "epoch: 5\n",
            "Loss [0/6250]: 1.4559036493301392\n",
            "Loss [100/6250]: 1.2030744552612305\n",
            "Loss [200/6250]: 0.6293197870254517\n",
            "Loss [300/6250]: 1.1040858030319214\n",
            "Loss [400/6250]: 0.6463567018508911\n",
            "Loss [500/6250]: 0.9283277988433838\n",
            "Loss [600/6250]: 0.8919679522514343\n",
            "Loss [700/6250]: 1.611170768737793\n",
            "Loss [800/6250]: 1.7367537021636963\n",
            "Loss [900/6250]: 0.7743421792984009\n",
            "Loss [1000/6250]: 1.2262349128723145\n",
            "Loss [1100/6250]: 0.45549866557121277\n",
            "Loss [1200/6250]: 1.1487454175949097\n",
            "Loss [1300/6250]: 0.7937145829200745\n",
            "Loss [1400/6250]: 0.40090835094451904\n",
            "Loss [1500/6250]: 1.180104374885559\n",
            "Loss [1600/6250]: 0.9008325934410095\n",
            "Loss [1700/6250]: 1.2258046865463257\n",
            "Loss [1800/6250]: 1.0254700183868408\n",
            "Loss [1900/6250]: 0.4418414831161499\n",
            "Loss [2000/6250]: 2.2863047122955322\n",
            "Loss [2100/6250]: 1.259019374847412\n",
            "Loss [2200/6250]: 1.0595918893814087\n",
            "Loss [2300/6250]: 0.8321255445480347\n",
            "Loss [2400/6250]: 1.3246264457702637\n",
            "Loss [2500/6250]: 0.93636155128479\n",
            "Loss [2600/6250]: 0.8645864129066467\n",
            "Loss [2700/6250]: 1.2917433977127075\n",
            "Loss [2800/6250]: 0.4585801362991333\n",
            "Loss [2900/6250]: 1.8903663158416748\n",
            "Loss [3000/6250]: 0.48820769786834717\n",
            "Loss [3100/6250]: 1.4400304555892944\n",
            "Loss [3200/6250]: 0.6981723308563232\n",
            "Loss [3300/6250]: 1.800415277481079\n",
            "Loss [3400/6250]: 1.1872586011886597\n",
            "Loss [3500/6250]: 1.2215571403503418\n",
            "Loss [3600/6250]: 0.6404788494110107\n",
            "Loss [3700/6250]: 1.6646136045455933\n",
            "Loss [3800/6250]: 0.9801740050315857\n",
            "Loss [3900/6250]: 1.0035300254821777\n",
            "Loss [4000/6250]: 0.9117920994758606\n",
            "Loss [4100/6250]: 0.8012409210205078\n",
            "Loss [4200/6250]: 0.5427647829055786\n",
            "Loss [4300/6250]: 0.9474172592163086\n",
            "Loss [4400/6250]: 0.7928259968757629\n",
            "Loss [4500/6250]: 1.07725191116333\n",
            "Loss [4600/6250]: 0.9618616104125977\n",
            "Loss [4700/6250]: 1.1418039798736572\n",
            "Loss [4800/6250]: 1.4560881853103638\n",
            "Loss [4900/6250]: 0.6417697668075562\n",
            "Loss [5000/6250]: 0.6284357309341431\n",
            "Loss [5100/6250]: 0.5319890975952148\n",
            "Loss [5200/6250]: 1.0813087224960327\n",
            "Loss [5300/6250]: 1.8457820415496826\n",
            "Loss [5400/6250]: 0.6346825361251831\n",
            "Loss [5500/6250]: 1.203722357749939\n",
            "Loss [5600/6250]: 0.8519214987754822\n",
            "Loss [5700/6250]: 1.3125135898590088\n",
            "Loss [5800/6250]: 1.090789794921875\n",
            "Loss [5900/6250]: 1.7300083637237549\n",
            "Loss [6000/6250]: 1.0862120389938354\n",
            "Loss [6100/6250]: 1.4818681478500366\n",
            "Loss [6200/6250]: 0.6605657935142517\n",
            "Train Accuracy: 60.553999999999995%\n",
            "Loss [0/1250]: 0.409675657749176\n",
            "Loss [100/1250]: 0.7389013767242432\n",
            "Loss [200/1250]: 1.1730045080184937\n",
            "Loss [300/1250]: 1.2746226787567139\n",
            "Loss [400/1250]: 1.097200870513916\n",
            "Loss [500/1250]: 1.4177488088607788\n",
            "Loss [600/1250]: 0.7168672680854797\n",
            "Loss [700/1250]: 1.2453384399414062\n",
            "Loss [800/1250]: 0.8583167195320129\n",
            "Loss [900/1250]: 1.1537697315216064\n",
            "Loss [1000/1250]: 1.225912094116211\n",
            "Loss [1100/1250]: 1.2645602226257324\n",
            "Loss [1200/1250]: 1.2837787866592407\n",
            "Test Accuracy: 64.38000000000001%\n",
            "epoch: 6\n",
            "Loss [0/6250]: 0.5900910496711731\n",
            "Loss [100/6250]: 1.2822985649108887\n",
            "Loss [200/6250]: 0.7908084988594055\n",
            "Loss [300/6250]: 1.5901684761047363\n",
            "Loss [400/6250]: 1.3517985343933105\n",
            "Loss [500/6250]: 0.9323272705078125\n",
            "Loss [600/6250]: 0.9014062285423279\n",
            "Loss [700/6250]: 1.0418769121170044\n",
            "Loss [800/6250]: 0.8374952673912048\n",
            "Loss [900/6250]: 1.0256563425064087\n",
            "Loss [1000/6250]: 1.0337181091308594\n",
            "Loss [1100/6250]: 1.6205421686172485\n",
            "Loss [1200/6250]: 0.6398825645446777\n",
            "Loss [1300/6250]: 0.8372185230255127\n",
            "Loss [1400/6250]: 1.015332579612732\n",
            "Loss [1500/6250]: 1.0587818622589111\n",
            "Loss [1600/6250]: 0.6625872254371643\n",
            "Loss [1700/6250]: 1.248874545097351\n",
            "Loss [1800/6250]: 0.8960964679718018\n",
            "Loss [1900/6250]: 0.3944458067417145\n",
            "Loss [2000/6250]: 0.48886483907699585\n",
            "Loss [2100/6250]: 0.7051548957824707\n",
            "Loss [2200/6250]: 1.6310573816299438\n",
            "Loss [2300/6250]: 0.8896245956420898\n",
            "Loss [2400/6250]: 0.8833007216453552\n",
            "Loss [2500/6250]: 0.7442301511764526\n",
            "Loss [2600/6250]: 0.7084259390830994\n",
            "Loss [2700/6250]: 1.2021374702453613\n",
            "Loss [2800/6250]: 0.8335586786270142\n",
            "Loss [2900/6250]: 1.5610228776931763\n",
            "Loss [3000/6250]: 0.7224608659744263\n",
            "Loss [3100/6250]: 1.520808458328247\n",
            "Loss [3200/6250]: 0.8957924842834473\n",
            "Loss [3300/6250]: 0.8683649897575378\n",
            "Loss [3400/6250]: 1.1978260278701782\n",
            "Loss [3500/6250]: 1.3812501430511475\n",
            "Loss [3600/6250]: 0.7824965119361877\n",
            "Loss [3700/6250]: 1.2936599254608154\n",
            "Loss [3800/6250]: 0.7919917702674866\n",
            "Loss [3900/6250]: 0.715979814529419\n",
            "Loss [4000/6250]: 0.9715798497200012\n",
            "Loss [4100/6250]: 0.5407634377479553\n",
            "Loss [4200/6250]: 0.4038936197757721\n",
            "Loss [4300/6250]: 1.2221840620040894\n",
            "Loss [4400/6250]: 0.6598400473594666\n",
            "Loss [4500/6250]: 1.1184085607528687\n",
            "Loss [4600/6250]: 1.7584285736083984\n",
            "Loss [4700/6250]: 0.4532569944858551\n",
            "Loss [4800/6250]: 0.8326007127761841\n",
            "Loss [4900/6250]: 1.4103387594223022\n",
            "Loss [5000/6250]: 1.2967816591262817\n",
            "Loss [5100/6250]: 1.3406116962432861\n",
            "Loss [5200/6250]: 0.6681541204452515\n",
            "Loss [5300/6250]: 0.655174195766449\n",
            "Loss [5400/6250]: 0.9715509414672852\n",
            "Loss [5500/6250]: 0.4442722201347351\n",
            "Loss [5600/6250]: 1.522574543952942\n",
            "Loss [5700/6250]: 2.0791726112365723\n",
            "Loss [5800/6250]: 1.2013111114501953\n",
            "Loss [5900/6250]: 1.4545480012893677\n",
            "Loss [6000/6250]: 0.7008935213088989\n",
            "Loss [6100/6250]: 0.7930605411529541\n",
            "Loss [6200/6250]: 2.1329493522644043\n",
            "Train Accuracy: 62.432%\n",
            "Loss [0/1250]: 0.8845975399017334\n",
            "Loss [100/1250]: 1.2996009588241577\n",
            "Loss [200/1250]: 1.4654967784881592\n",
            "Loss [300/1250]: 1.228540062904358\n",
            "Loss [400/1250]: 0.8817276954650879\n",
            "Loss [500/1250]: 1.2943294048309326\n",
            "Loss [600/1250]: 0.8571943640708923\n",
            "Loss [700/1250]: 1.4294887781143188\n",
            "Loss [800/1250]: 1.1455439329147339\n",
            "Loss [900/1250]: 1.1481770277023315\n",
            "Loss [1000/1250]: 1.1985175609588623\n",
            "Loss [1100/1250]: 1.1666016578674316\n",
            "Loss [1200/1250]: 1.0698593854904175\n",
            "Test Accuracy: 66.03999999999999%\n",
            "epoch: 7\n",
            "Loss [0/6250]: 0.594179630279541\n",
            "Loss [100/6250]: 1.0143895149230957\n",
            "Loss [200/6250]: 0.5237627029418945\n",
            "Loss [300/6250]: 1.138305902481079\n",
            "Loss [400/6250]: 0.7302244901657104\n",
            "Loss [500/6250]: 1.6047770977020264\n",
            "Loss [600/6250]: 0.7631877660751343\n",
            "Loss [700/6250]: 1.0239996910095215\n",
            "Loss [800/6250]: 1.6102268695831299\n",
            "Loss [900/6250]: 0.9820461273193359\n",
            "Loss [1000/6250]: 0.6841671466827393\n",
            "Loss [1100/6250]: 0.791916012763977\n",
            "Loss [1200/6250]: 0.6520890593528748\n",
            "Loss [1300/6250]: 1.201261043548584\n",
            "Loss [1400/6250]: 0.9165934324264526\n",
            "Loss [1500/6250]: 0.764133632183075\n",
            "Loss [1600/6250]: 1.3992639780044556\n",
            "Loss [1700/6250]: 0.7756080031394958\n",
            "Loss [1800/6250]: 1.0929594039916992\n",
            "Loss [1900/6250]: 0.7733572721481323\n",
            "Loss [2000/6250]: 1.575877070426941\n",
            "Loss [2100/6250]: 1.2988804578781128\n",
            "Loss [2200/6250]: 1.1058920621871948\n",
            "Loss [2300/6250]: 0.679980993270874\n",
            "Loss [2400/6250]: 0.8350954055786133\n",
            "Loss [2500/6250]: 0.8532993197441101\n",
            "Loss [2600/6250]: 0.5767396688461304\n",
            "Loss [2700/6250]: 0.36223331093788147\n",
            "Loss [2800/6250]: 2.1222352981567383\n",
            "Loss [2900/6250]: 1.42317533493042\n",
            "Loss [3000/6250]: 1.0248178243637085\n",
            "Loss [3100/6250]: 0.6828751564025879\n",
            "Loss [3200/6250]: 0.5302650332450867\n",
            "Loss [3300/6250]: 0.6280816793441772\n",
            "Loss [3400/6250]: 1.237380027770996\n",
            "Loss [3500/6250]: 1.43682861328125\n",
            "Loss [3600/6250]: 0.7244658470153809\n",
            "Loss [3700/6250]: 1.4682029485702515\n",
            "Loss [3800/6250]: 0.18413040041923523\n",
            "Loss [3900/6250]: 0.6987709999084473\n",
            "Loss [4000/6250]: 1.35004460811615\n",
            "Loss [4100/6250]: 0.7908538579940796\n",
            "Loss [4200/6250]: 0.9274600148200989\n",
            "Loss [4300/6250]: 0.6723360419273376\n",
            "Loss [4400/6250]: 0.5986453294754028\n",
            "Loss [4500/6250]: 0.4706602990627289\n",
            "Loss [4600/6250]: 0.49491363763809204\n",
            "Loss [4700/6250]: 0.9288858771324158\n",
            "Loss [4800/6250]: 0.3562498986721039\n",
            "Loss [4900/6250]: 0.6329420208930969\n",
            "Loss [5000/6250]: 0.4309930205345154\n",
            "Loss [5100/6250]: 0.8401922583580017\n",
            "Loss [5200/6250]: 0.4643731415271759\n",
            "Loss [5300/6250]: 0.9272648096084595\n",
            "Loss [5400/6250]: 1.7695835828781128\n",
            "Loss [5500/6250]: 0.909822940826416\n",
            "Loss [5600/6250]: 0.37272176146507263\n",
            "Loss [5700/6250]: 1.4944757223129272\n",
            "Loss [5800/6250]: 1.1239022016525269\n",
            "Loss [5900/6250]: 0.6612523198127747\n",
            "Loss [6000/6250]: 0.6273612380027771\n",
            "Loss [6100/6250]: 1.556113600730896\n",
            "Loss [6200/6250]: 1.236281394958496\n",
            "Train Accuracy: 64.076%\n",
            "Loss [0/1250]: 0.6075130701065063\n",
            "Loss [100/1250]: 1.1699419021606445\n",
            "Loss [200/1250]: 1.3604438304901123\n",
            "Loss [300/1250]: 1.6947267055511475\n",
            "Loss [400/1250]: 1.0451915264129639\n",
            "Loss [500/1250]: 1.740424633026123\n",
            "Loss [600/1250]: 0.6373262405395508\n",
            "Loss [700/1250]: 1.0254995822906494\n",
            "Loss [800/1250]: 1.204647183418274\n",
            "Loss [900/1250]: 1.2413933277130127\n",
            "Loss [1000/1250]: 0.8918445110321045\n",
            "Loss [1100/1250]: 1.0792075395584106\n",
            "Loss [1200/1250]: 1.2287214994430542\n",
            "Test Accuracy: 66.88%\n",
            "epoch: 8\n",
            "Loss [0/6250]: 0.9238645434379578\n",
            "Loss [100/6250]: 0.8647458553314209\n",
            "Loss [200/6250]: 1.2443947792053223\n",
            "Loss [300/6250]: 0.7205080389976501\n",
            "Loss [400/6250]: 0.8236298561096191\n",
            "Loss [500/6250]: 0.5978138446807861\n",
            "Loss [600/6250]: 2.3224234580993652\n",
            "Loss [700/6250]: 1.0609055757522583\n",
            "Loss [800/6250]: 1.1014893054962158\n",
            "Loss [900/6250]: 0.8169106841087341\n",
            "Loss [1000/6250]: 0.4735211730003357\n",
            "Loss [1100/6250]: 0.9271140098571777\n",
            "Loss [1200/6250]: 0.618926465511322\n",
            "Loss [1300/6250]: 0.8770785331726074\n",
            "Loss [1400/6250]: 1.2532720565795898\n",
            "Loss [1500/6250]: 1.420637845993042\n",
            "Loss [1600/6250]: 1.0308648347854614\n",
            "Loss [1700/6250]: 0.634418249130249\n",
            "Loss [1800/6250]: 1.5689493417739868\n",
            "Loss [1900/6250]: 1.1796936988830566\n",
            "Loss [2000/6250]: 1.2510747909545898\n",
            "Loss [2100/6250]: 0.5788789987564087\n",
            "Loss [2200/6250]: 1.0296196937561035\n",
            "Loss [2300/6250]: 0.7518560290336609\n",
            "Loss [2400/6250]: 1.1874463558197021\n",
            "Loss [2500/6250]: 0.7296962141990662\n",
            "Loss [2600/6250]: 1.313280701637268\n",
            "Loss [2700/6250]: 1.0214996337890625\n",
            "Loss [2800/6250]: 0.6154898405075073\n",
            "Loss [2900/6250]: 0.7199280858039856\n",
            "Loss [3000/6250]: 1.015203595161438\n",
            "Loss [3100/6250]: 0.9827672243118286\n",
            "Loss [3200/6250]: 1.1036949157714844\n",
            "Loss [3300/6250]: 1.201645016670227\n",
            "Loss [3400/6250]: 0.5110716819763184\n",
            "Loss [3500/6250]: 0.8778492212295532\n",
            "Loss [3600/6250]: 0.919380784034729\n",
            "Loss [3700/6250]: 1.2537667751312256\n",
            "Loss [3800/6250]: 1.0995960235595703\n",
            "Loss [3900/6250]: 0.5513225793838501\n",
            "Loss [4000/6250]: 1.20961594581604\n",
            "Loss [4100/6250]: 0.5905311107635498\n",
            "Loss [4200/6250]: 1.3808952569961548\n",
            "Loss [4300/6250]: 0.8211348056793213\n",
            "Loss [4400/6250]: 1.4285398721694946\n",
            "Loss [4500/6250]: 1.2963448762893677\n",
            "Loss [4600/6250]: 0.7271124720573425\n",
            "Loss [4700/6250]: 1.476736068725586\n",
            "Loss [4800/6250]: 1.1347055435180664\n",
            "Loss [4900/6250]: 0.8877018690109253\n",
            "Loss [5000/6250]: 0.7733862400054932\n",
            "Loss [5100/6250]: 1.2030166387557983\n",
            "Loss [5200/6250]: 0.5893233418464661\n",
            "Loss [5300/6250]: 0.5639969110488892\n",
            "Loss [5400/6250]: 0.5592111349105835\n",
            "Loss [5500/6250]: 1.139309287071228\n",
            "Loss [5600/6250]: 0.859966516494751\n",
            "Loss [5700/6250]: 1.6129952669143677\n",
            "Loss [5800/6250]: 0.9863008856773376\n",
            "Loss [5900/6250]: 1.2689104080200195\n",
            "Loss [6000/6250]: 1.3564692735671997\n",
            "Loss [6100/6250]: 1.5600852966308594\n",
            "Loss [6200/6250]: 0.5711011290550232\n",
            "Train Accuracy: 65.854%\n",
            "Loss [0/1250]: 0.4354285001754761\n",
            "Loss [100/1250]: 0.5613815188407898\n",
            "Loss [200/1250]: 1.1492253541946411\n",
            "Loss [300/1250]: 1.1144986152648926\n",
            "Loss [400/1250]: 1.0609925985336304\n",
            "Loss [500/1250]: 1.3580118417739868\n",
            "Loss [600/1250]: 0.6184839606285095\n",
            "Loss [700/1250]: 1.1480324268341064\n",
            "Loss [800/1250]: 0.864290177822113\n",
            "Loss [900/1250]: 1.3720040321350098\n",
            "Loss [1000/1250]: 0.60176020860672\n",
            "Loss [1100/1250]: 0.7844643592834473\n",
            "Loss [1200/1250]: 1.3650383949279785\n",
            "Test Accuracy: 68.22%\n",
            "epoch: 9\n",
            "Loss [0/6250]: 0.36666637659072876\n",
            "Loss [100/6250]: 1.147129774093628\n",
            "Loss [200/6250]: 1.2487542629241943\n",
            "Loss [300/6250]: 0.9243270754814148\n",
            "Loss [400/6250]: 0.6475726366043091\n",
            "Loss [500/6250]: 0.935570240020752\n",
            "Loss [600/6250]: 1.103249192237854\n",
            "Loss [700/6250]: 1.1510274410247803\n",
            "Loss [800/6250]: 0.3012801706790924\n",
            "Loss [900/6250]: 1.1403450965881348\n",
            "Loss [1000/6250]: 0.9619567394256592\n",
            "Loss [1100/6250]: 0.6982091665267944\n",
            "Loss [1200/6250]: 0.7397330403327942\n",
            "Loss [1300/6250]: 0.761824369430542\n",
            "Loss [1400/6250]: 1.1582157611846924\n",
            "Loss [1500/6250]: 0.33582887053489685\n",
            "Loss [1600/6250]: 1.1584457159042358\n",
            "Loss [1700/6250]: 0.8293852806091309\n",
            "Loss [1800/6250]: 1.6394542455673218\n",
            "Loss [1900/6250]: 0.795468807220459\n",
            "Loss [2000/6250]: 0.8912198543548584\n",
            "Loss [2100/6250]: 0.6547955870628357\n",
            "Loss [2200/6250]: 1.297780156135559\n",
            "Loss [2300/6250]: 0.29467928409576416\n",
            "Loss [2400/6250]: 1.3902339935302734\n",
            "Loss [2500/6250]: 2.101064682006836\n",
            "Loss [2600/6250]: 0.47178947925567627\n",
            "Loss [2700/6250]: 0.4852752089500427\n",
            "Loss [2800/6250]: 0.573418915271759\n",
            "Loss [2900/6250]: 0.9989883303642273\n",
            "Loss [3000/6250]: 1.934891700744629\n",
            "Loss [3100/6250]: 0.5501096844673157\n",
            "Loss [3200/6250]: 0.6013926863670349\n",
            "Loss [3300/6250]: 0.6753031611442566\n",
            "Loss [3400/6250]: 0.6395259499549866\n",
            "Loss [3500/6250]: 0.6063980460166931\n",
            "Loss [3600/6250]: 0.8276230692863464\n",
            "Loss [3700/6250]: 0.7571581602096558\n",
            "Loss [3800/6250]: 0.9370335340499878\n",
            "Loss [3900/6250]: 0.44683030247688293\n",
            "Loss [4000/6250]: 0.34141018986701965\n",
            "Loss [4100/6250]: 0.9246820211410522\n",
            "Loss [4200/6250]: 0.9821738004684448\n",
            "Loss [4300/6250]: 0.5340496897697449\n",
            "Loss [4400/6250]: 0.5218719840049744\n",
            "Loss [4500/6250]: 0.7851225137710571\n",
            "Loss [4600/6250]: 1.262915849685669\n",
            "Loss [4700/6250]: 0.9336168169975281\n",
            "Loss [4800/6250]: 0.6889498829841614\n",
            "Loss [4900/6250]: 1.3300890922546387\n",
            "Loss [5000/6250]: 0.8521757125854492\n",
            "Loss [5100/6250]: 1.2904565334320068\n",
            "Loss [5200/6250]: 0.8740034699440002\n",
            "Loss [5300/6250]: 0.586588442325592\n",
            "Loss [5400/6250]: 0.6544931530952454\n",
            "Loss [5500/6250]: 0.39555758237838745\n",
            "Loss [5600/6250]: 0.7023720741271973\n",
            "Loss [5700/6250]: 1.0039892196655273\n",
            "Loss [5800/6250]: 1.1200398206710815\n",
            "Loss [5900/6250]: 0.6826345324516296\n",
            "Loss [6000/6250]: 1.3767211437225342\n",
            "Loss [6100/6250]: 0.48499321937561035\n",
            "Loss [6200/6250]: 1.611435890197754\n",
            "Train Accuracy: 66.986%\n",
            "Loss [0/1250]: 0.41843488812446594\n",
            "Loss [100/1250]: 0.849822461605072\n",
            "Loss [200/1250]: 1.0331788063049316\n",
            "Loss [300/1250]: 1.107422947883606\n",
            "Loss [400/1250]: 1.3187451362609863\n",
            "Loss [500/1250]: 1.298722743988037\n",
            "Loss [600/1250]: 0.7199003100395203\n",
            "Loss [700/1250]: 1.1226508617401123\n",
            "Loss [800/1250]: 0.8528914451599121\n",
            "Loss [900/1250]: 1.2278772592544556\n",
            "Loss [1000/1250]: 0.6729462146759033\n",
            "Loss [1100/1250]: 1.2082611322402954\n",
            "Loss [1200/1250]: 1.7497615814208984\n",
            "Test Accuracy: 68.58%\n",
            "epoch: 10\n",
            "Loss [0/6250]: 0.5557101964950562\n",
            "Loss [100/6250]: 1.2673593759536743\n",
            "Loss [200/6250]: 1.6351280212402344\n",
            "Loss [300/6250]: 1.1278239488601685\n",
            "Loss [400/6250]: 1.117457389831543\n",
            "Loss [500/6250]: 0.6261926889419556\n",
            "Loss [600/6250]: 0.9289597868919373\n",
            "Loss [700/6250]: 1.2863906621932983\n",
            "Loss [800/6250]: 0.9938837885856628\n",
            "Loss [900/6250]: 0.647821307182312\n",
            "Loss [1000/6250]: 0.634920060634613\n",
            "Loss [1100/6250]: 0.363540917634964\n",
            "Loss [1200/6250]: 0.33560726046562195\n",
            "Loss [1300/6250]: 1.1247093677520752\n",
            "Loss [1400/6250]: 0.3401276171207428\n",
            "Loss [1500/6250]: 0.8662809133529663\n",
            "Loss [1600/6250]: 0.6069359183311462\n",
            "Loss [1700/6250]: 0.6470380425453186\n",
            "Loss [1800/6250]: 1.7623329162597656\n",
            "Loss [1900/6250]: 0.6600313782691956\n",
            "Loss [2000/6250]: 1.4354521036148071\n",
            "Loss [2100/6250]: 1.5915658473968506\n",
            "Loss [2200/6250]: 0.6300275921821594\n",
            "Loss [2300/6250]: 0.8654724955558777\n",
            "Loss [2400/6250]: 0.8230647444725037\n",
            "Loss [2500/6250]: 0.7609920501708984\n",
            "Loss [2600/6250]: 0.40537333488464355\n",
            "Loss [2700/6250]: 1.2280324697494507\n",
            "Loss [2800/6250]: 0.9375236630439758\n",
            "Loss [2900/6250]: 0.907541811466217\n",
            "Loss [3000/6250]: 1.131985068321228\n",
            "Loss [3100/6250]: 0.8413594365119934\n",
            "Loss [3200/6250]: 1.1727190017700195\n",
            "Loss [3300/6250]: 0.22371868789196014\n",
            "Loss [3400/6250]: 0.6653842329978943\n",
            "Loss [3500/6250]: 0.9864041209220886\n",
            "Loss [3600/6250]: 0.720367431640625\n",
            "Loss [3700/6250]: 0.37601202726364136\n",
            "Loss [3800/6250]: 0.822381854057312\n",
            "Loss [3900/6250]: 1.6312682628631592\n",
            "Loss [4000/6250]: 1.2613067626953125\n",
            "Loss [4100/6250]: 1.8667762279510498\n",
            "Loss [4200/6250]: 1.2408943176269531\n",
            "Loss [4300/6250]: 0.4628475308418274\n",
            "Loss [4400/6250]: 1.3133686780929565\n",
            "Loss [4500/6250]: 1.480189323425293\n",
            "Loss [4600/6250]: 1.3338640928268433\n",
            "Loss [4700/6250]: 1.5337941646575928\n",
            "Loss [4800/6250]: 1.153790831565857\n",
            "Loss [4900/6250]: 1.2735404968261719\n",
            "Loss [5000/6250]: 1.472129464149475\n",
            "Loss [5100/6250]: 1.2208009958267212\n",
            "Loss [5200/6250]: 1.0560733079910278\n",
            "Loss [5300/6250]: 0.5526484847068787\n",
            "Loss [5400/6250]: 1.6301323175430298\n",
            "Loss [5500/6250]: 0.9791751503944397\n",
            "Loss [5600/6250]: 1.3437162637710571\n",
            "Loss [5700/6250]: 0.5480839610099792\n",
            "Loss [5800/6250]: 1.3097913265228271\n",
            "Loss [5900/6250]: 0.9973148107528687\n",
            "Loss [6000/6250]: 1.2388287782669067\n",
            "Loss [6100/6250]: 0.6339831948280334\n",
            "Loss [6200/6250]: 0.7369514107704163\n",
            "Train Accuracy: 68.074%\n",
            "Loss [0/1250]: 0.34807851910591125\n",
            "Loss [100/1250]: 1.4232841730117798\n",
            "Loss [200/1250]: 1.2893410921096802\n",
            "Loss [300/1250]: 1.218961477279663\n",
            "Loss [400/1250]: 1.12279212474823\n",
            "Loss [500/1250]: 1.4929708242416382\n",
            "Loss [600/1250]: 0.7117637991905212\n",
            "Loss [700/1250]: 0.8653992414474487\n",
            "Loss [800/1250]: 0.7228129506111145\n",
            "Loss [900/1250]: 1.085310935974121\n",
            "Loss [1000/1250]: 0.4710742235183716\n",
            "Loss [1100/1250]: 1.1568139791488647\n",
            "Loss [1200/1250]: 1.4029560089111328\n",
            "Test Accuracy: 71.16%\n",
            "epoch: 11\n",
            "Loss [0/6250]: 0.6947309374809265\n",
            "Loss [100/6250]: 0.8758643269538879\n",
            "Loss [200/6250]: 0.6832064986228943\n",
            "Loss [300/6250]: 1.1715726852416992\n",
            "Loss [400/6250]: 1.395790934562683\n",
            "Loss [500/6250]: 1.098854422569275\n",
            "Loss [600/6250]: 0.7626442909240723\n",
            "Loss [700/6250]: 1.72959566116333\n",
            "Loss [800/6250]: 1.4830374717712402\n",
            "Loss [900/6250]: 0.7406482696533203\n",
            "Loss [1000/6250]: 1.0807050466537476\n",
            "Loss [1100/6250]: 0.9579119086265564\n",
            "Loss [1200/6250]: 0.5402792096138\n",
            "Loss [1300/6250]: 1.0937402248382568\n",
            "Loss [1400/6250]: 0.47260022163391113\n",
            "Loss [1500/6250]: 0.8539432287216187\n",
            "Loss [1600/6250]: 0.3002520501613617\n",
            "Loss [1700/6250]: 1.1028274297714233\n",
            "Loss [1800/6250]: 0.713172972202301\n",
            "Loss [1900/6250]: 1.0463908910751343\n",
            "Loss [2000/6250]: 1.1196662187576294\n",
            "Loss [2100/6250]: 0.6515342593193054\n",
            "Loss [2200/6250]: 0.3705805838108063\n",
            "Loss [2300/6250]: 0.9678637981414795\n",
            "Loss [2400/6250]: 0.7249887585639954\n",
            "Loss [2500/6250]: 1.1227163076400757\n",
            "Loss [2600/6250]: 0.4301147162914276\n",
            "Loss [2700/6250]: 0.18350760638713837\n",
            "Loss [2800/6250]: 0.705226719379425\n",
            "Loss [2900/6250]: 1.1468706130981445\n",
            "Loss [3000/6250]: 0.985458493232727\n",
            "Loss [3100/6250]: 0.9376623034477234\n",
            "Loss [3200/6250]: 0.8015236258506775\n",
            "Loss [3300/6250]: 0.6551865339279175\n",
            "Loss [3400/6250]: 0.5435754060745239\n",
            "Loss [3500/6250]: 0.5606868267059326\n",
            "Loss [3600/6250]: 0.29593273997306824\n",
            "Loss [3700/6250]: 0.5416091084480286\n",
            "Loss [3800/6250]: 0.7588513493537903\n",
            "Loss [3900/6250]: 0.7733082175254822\n",
            "Loss [4000/6250]: 1.2112622261047363\n",
            "Loss [4100/6250]: 0.9703478217124939\n",
            "Loss [4200/6250]: 0.3932220935821533\n",
            "Loss [4300/6250]: 0.6115995645523071\n",
            "Loss [4400/6250]: 0.8546880483627319\n",
            "Loss [4500/6250]: 0.3107140064239502\n",
            "Loss [4600/6250]: 1.065870761871338\n",
            "Loss [4700/6250]: 1.2294328212738037\n",
            "Loss [4800/6250]: 0.8044659495353699\n",
            "Loss [4900/6250]: 1.2090351581573486\n",
            "Loss [5000/6250]: 1.4025567770004272\n",
            "Loss [5100/6250]: 1.0328062772750854\n",
            "Loss [5200/6250]: 0.5183252692222595\n",
            "Loss [5300/6250]: 1.0011037588119507\n",
            "Loss [5400/6250]: 0.7099817395210266\n",
            "Loss [5500/6250]: 1.3502581119537354\n",
            "Loss [5600/6250]: 0.8178662657737732\n",
            "Loss [5700/6250]: 0.22688187658786774\n",
            "Loss [5800/6250]: 1.2352186441421509\n",
            "Loss [5900/6250]: 1.5406897068023682\n",
            "Loss [6000/6250]: 0.21174439787864685\n",
            "Loss [6100/6250]: 1.1047974824905396\n",
            "Loss [6200/6250]: 0.5731672048568726\n",
            "Train Accuracy: 69.00800000000001%\n",
            "Loss [0/1250]: 0.7329678535461426\n",
            "Loss [100/1250]: 1.0860308408737183\n",
            "Loss [200/1250]: 1.0022279024124146\n",
            "Loss [300/1250]: 1.1495627164840698\n",
            "Loss [400/1250]: 0.6697648763656616\n",
            "Loss [500/1250]: 1.1938245296478271\n",
            "Loss [600/1250]: 0.949672281742096\n",
            "Loss [700/1250]: 1.323395013809204\n",
            "Loss [800/1250]: 0.8936694860458374\n",
            "Loss [900/1250]: 1.376347541809082\n",
            "Loss [1000/1250]: 0.8186581134796143\n",
            "Loss [1100/1250]: 0.9257315993309021\n",
            "Loss [1200/1250]: 1.604317545890808\n",
            "Test Accuracy: 71.75%\n",
            "epoch: 12\n",
            "Loss [0/6250]: 1.331707239151001\n",
            "Loss [100/6250]: 0.8705045580863953\n",
            "Loss [200/6250]: 1.0944485664367676\n",
            "Loss [300/6250]: 2.273469924926758\n",
            "Loss [400/6250]: 1.3520081043243408\n",
            "Loss [500/6250]: 0.20581528544425964\n",
            "Loss [600/6250]: 0.9363164901733398\n",
            "Loss [700/6250]: 0.7035605311393738\n",
            "Loss [800/6250]: 0.6428602337837219\n",
            "Loss [900/6250]: 0.6687785983085632\n",
            "Loss [1000/6250]: 1.149771809577942\n",
            "Loss [1100/6250]: 0.7853416204452515\n",
            "Loss [1200/6250]: 0.518139123916626\n",
            "Loss [1300/6250]: 0.4565797448158264\n",
            "Loss [1400/6250]: 1.1638774871826172\n",
            "Loss [1500/6250]: 0.7316384315490723\n",
            "Loss [1600/6250]: 0.7872642874717712\n",
            "Loss [1700/6250]: 0.8411523699760437\n",
            "Loss [1800/6250]: 1.3304483890533447\n",
            "Loss [1900/6250]: 1.3119943141937256\n",
            "Loss [2000/6250]: 0.5115081071853638\n",
            "Loss [2100/6250]: 0.8893268704414368\n",
            "Loss [2200/6250]: 0.7591860294342041\n",
            "Loss [2300/6250]: 0.5406293869018555\n",
            "Loss [2400/6250]: 0.40720510482788086\n",
            "Loss [2500/6250]: 0.7902817130088806\n",
            "Loss [2600/6250]: 0.3038370609283447\n",
            "Loss [2700/6250]: 0.8588389158248901\n",
            "Loss [2800/6250]: 1.1586796045303345\n",
            "Loss [2900/6250]: 0.8587782979011536\n",
            "Loss [3000/6250]: 0.4864875376224518\n",
            "Loss [3100/6250]: 0.8040130138397217\n",
            "Loss [3200/6250]: 1.7694785594940186\n",
            "Loss [3300/6250]: 0.7079066038131714\n",
            "Loss [3400/6250]: 0.806749701499939\n",
            "Loss [3500/6250]: 0.5148584246635437\n",
            "Loss [3600/6250]: 0.9281574487686157\n",
            "Loss [3700/6250]: 1.4092532396316528\n",
            "Loss [3800/6250]: 0.6239700317382812\n",
            "Loss [3900/6250]: 0.9311618804931641\n",
            "Loss [4000/6250]: 0.6044895648956299\n",
            "Loss [4100/6250]: 0.9737135171890259\n",
            "Loss [4200/6250]: 0.21570205688476562\n",
            "Loss [4300/6250]: 0.5848347544670105\n",
            "Loss [4400/6250]: 0.9624525308609009\n",
            "Loss [4500/6250]: 0.8518421053886414\n",
            "Loss [4600/6250]: 1.192063570022583\n",
            "Loss [4700/6250]: 0.7055872082710266\n",
            "Loss [4800/6250]: 1.3932054042816162\n",
            "Loss [4900/6250]: 1.6258764266967773\n",
            "Loss [5000/6250]: 0.28286293148994446\n",
            "Loss [5100/6250]: 0.9224470853805542\n",
            "Loss [5200/6250]: 0.761584997177124\n",
            "Loss [5300/6250]: 0.3129730224609375\n",
            "Loss [5400/6250]: 0.2585550546646118\n",
            "Loss [5500/6250]: 0.30519747734069824\n",
            "Loss [5600/6250]: 0.4207909107208252\n",
            "Loss [5700/6250]: 1.030336618423462\n",
            "Loss [5800/6250]: 0.609386146068573\n",
            "Loss [5900/6250]: 0.27863967418670654\n",
            "Loss [6000/6250]: 1.2873307466506958\n",
            "Loss [6100/6250]: 0.4313659071922302\n",
            "Loss [6200/6250]: 0.9735963344573975\n",
            "Train Accuracy: 70.096%\n",
            "Loss [0/1250]: 0.586259663105011\n",
            "Loss [100/1250]: 0.7328177690505981\n",
            "Loss [200/1250]: 1.3060390949249268\n",
            "Loss [300/1250]: 0.7883213758468628\n",
            "Loss [400/1250]: 1.0276561975479126\n",
            "Loss [500/1250]: 1.5094770193099976\n",
            "Loss [600/1250]: 0.6154625415802002\n",
            "Loss [700/1250]: 1.0447514057159424\n",
            "Loss [800/1250]: 0.6311818361282349\n",
            "Loss [900/1250]: 1.0984684228897095\n",
            "Loss [1000/1250]: 0.8576882481575012\n",
            "Loss [1100/1250]: 0.7713070511817932\n",
            "Loss [1200/1250]: 1.2361563444137573\n",
            "Test Accuracy: 71.83%\n",
            "epoch: 13\n",
            "Loss [0/6250]: 0.4110835790634155\n",
            "Loss [100/6250]: 0.8270465731620789\n",
            "Loss [200/6250]: 0.3367212414741516\n",
            "Loss [300/6250]: 1.1018649339675903\n",
            "Loss [400/6250]: 0.6966550946235657\n",
            "Loss [500/6250]: 0.4687475562095642\n",
            "Loss [600/6250]: 0.7651607394218445\n",
            "Loss [700/6250]: 0.7704818248748779\n",
            "Loss [800/6250]: 0.5429507493972778\n",
            "Loss [900/6250]: 0.6812167763710022\n",
            "Loss [1000/6250]: 1.423040509223938\n",
            "Loss [1100/6250]: 1.0205028057098389\n",
            "Loss [1200/6250]: 1.4170916080474854\n",
            "Loss [1300/6250]: 0.23698002099990845\n",
            "Loss [1400/6250]: 2.1704261302948\n",
            "Loss [1500/6250]: 1.0614029169082642\n",
            "Loss [1600/6250]: 0.710210919380188\n",
            "Loss [1700/6250]: 0.46654292941093445\n",
            "Loss [1800/6250]: 1.2156020402908325\n",
            "Loss [1900/6250]: 1.2899677753448486\n",
            "Loss [2000/6250]: 0.7638594508171082\n",
            "Loss [2100/6250]: 0.37988176941871643\n",
            "Loss [2200/6250]: 0.5728212594985962\n",
            "Loss [2300/6250]: 0.525980532169342\n",
            "Loss [2400/6250]: 0.40584757924079895\n",
            "Loss [2500/6250]: 0.3466629683971405\n",
            "Loss [2600/6250]: 0.3138616979122162\n",
            "Loss [2700/6250]: 0.7321738004684448\n",
            "Loss [2800/6250]: 0.3182741403579712\n",
            "Loss [2900/6250]: 0.9827871322631836\n",
            "Loss [3000/6250]: 2.0755884647369385\n",
            "Loss [3100/6250]: 1.5531599521636963\n",
            "Loss [3200/6250]: 0.2011100947856903\n",
            "Loss [3300/6250]: 0.540789783000946\n",
            "Loss [3400/6250]: 0.825691282749176\n",
            "Loss [3500/6250]: 0.9508695006370544\n",
            "Loss [3600/6250]: 0.3264176547527313\n",
            "Loss [3700/6250]: 0.8598936200141907\n",
            "Loss [3800/6250]: 0.7096373438835144\n",
            "Loss [3900/6250]: 1.329391598701477\n",
            "Loss [4000/6250]: 1.1461080312728882\n",
            "Loss [4100/6250]: 0.5705223083496094\n",
            "Loss [4200/6250]: 1.0749495029449463\n",
            "Loss [4300/6250]: 0.5682839751243591\n",
            "Loss [4400/6250]: 0.6239898800849915\n",
            "Loss [4500/6250]: 0.982925295829773\n",
            "Loss [4600/6250]: 0.3878988027572632\n",
            "Loss [4700/6250]: 1.1876513957977295\n",
            "Loss [4800/6250]: 0.63007652759552\n",
            "Loss [4900/6250]: 0.8226493000984192\n",
            "Loss [5000/6250]: 1.1187106370925903\n",
            "Loss [5100/6250]: 0.5384669899940491\n",
            "Loss [5200/6250]: 0.782140851020813\n",
            "Loss [5300/6250]: 1.0450656414031982\n",
            "Loss [5400/6250]: 0.7630524635314941\n",
            "Loss [5500/6250]: 1.51470947265625\n",
            "Loss [5600/6250]: 0.9454124569892883\n",
            "Loss [5700/6250]: 0.8060151934623718\n",
            "Loss [5800/6250]: 1.1061103343963623\n",
            "Loss [5900/6250]: 0.5133563876152039\n",
            "Loss [6000/6250]: 0.7382673621177673\n",
            "Loss [6100/6250]: 0.8238799571990967\n",
            "Loss [6200/6250]: 0.9061314463615417\n",
            "Train Accuracy: 71.222%\n",
            "Loss [0/1250]: 0.7291033267974854\n",
            "Loss [100/1250]: 0.7439820766448975\n",
            "Loss [200/1250]: 0.8702892065048218\n",
            "Loss [300/1250]: 0.645906388759613\n",
            "Loss [400/1250]: 0.9856209754943848\n",
            "Loss [500/1250]: 1.2858359813690186\n",
            "Loss [600/1250]: 0.5639630556106567\n",
            "Loss [700/1250]: 1.2246166467666626\n",
            "Loss [800/1250]: 1.0082855224609375\n",
            "Loss [900/1250]: 0.8089661002159119\n",
            "Loss [1000/1250]: 0.7362946271896362\n",
            "Loss [1100/1250]: 1.202494502067566\n",
            "Loss [1200/1250]: 1.5302231311798096\n",
            "Test Accuracy: 71.74000000000001%\n",
            "epoch: 14\n",
            "Loss [0/6250]: 1.028590440750122\n",
            "Loss [100/6250]: 1.0353527069091797\n",
            "Loss [200/6250]: 0.5823523998260498\n",
            "Loss [300/6250]: 0.757817804813385\n",
            "Loss [400/6250]: 1.010703206062317\n",
            "Loss [500/6250]: 0.802343487739563\n",
            "Loss [600/6250]: 1.596661925315857\n",
            "Loss [700/6250]: 0.7154922485351562\n",
            "Loss [800/6250]: 1.586746335029602\n",
            "Loss [900/6250]: 0.3082599341869354\n",
            "Loss [1000/6250]: 0.47851642966270447\n",
            "Loss [1100/6250]: 0.47954055666923523\n",
            "Loss [1200/6250]: 0.6551605463027954\n",
            "Loss [1300/6250]: 0.364174485206604\n",
            "Loss [1400/6250]: 0.6088661551475525\n",
            "Loss [1500/6250]: 0.28427574038505554\n",
            "Loss [1600/6250]: 1.0173856019973755\n",
            "Loss [1700/6250]: 0.30785179138183594\n",
            "Loss [1800/6250]: 0.30935508012771606\n",
            "Loss [1900/6250]: 0.5651267170906067\n",
            "Loss [2000/6250]: 0.5490479469299316\n",
            "Loss [2100/6250]: 0.8863208889961243\n",
            "Loss [2200/6250]: 0.8329019546508789\n",
            "Loss [2300/6250]: 1.3012995719909668\n",
            "Loss [2400/6250]: 0.7585663199424744\n",
            "Loss [2500/6250]: 0.591431736946106\n",
            "Loss [2600/6250]: 0.5945851802825928\n",
            "Loss [2700/6250]: 0.2366856336593628\n",
            "Loss [2800/6250]: 0.7065988183021545\n",
            "Loss [2900/6250]: 0.71048903465271\n",
            "Loss [3000/6250]: 0.9058446288108826\n",
            "Loss [3100/6250]: 1.0684986114501953\n",
            "Loss [3200/6250]: 1.038235068321228\n",
            "Loss [3300/6250]: 1.4413787126541138\n",
            "Loss [3400/6250]: 1.0964276790618896\n",
            "Loss [3500/6250]: 1.5187450647354126\n",
            "Loss [3600/6250]: 0.8343976140022278\n",
            "Loss [3700/6250]: 1.5498358011245728\n",
            "Loss [3800/6250]: 0.6513986587524414\n",
            "Loss [3900/6250]: 0.3813445270061493\n",
            "Loss [4000/6250]: 0.4619274139404297\n",
            "Loss [4100/6250]: 0.7760119438171387\n",
            "Loss [4200/6250]: 0.7875692248344421\n",
            "Loss [4300/6250]: 0.5339409708976746\n",
            "Loss [4400/6250]: 0.7851301431655884\n",
            "Loss [4500/6250]: 0.537329912185669\n",
            "Loss [4600/6250]: 0.7910641431808472\n",
            "Loss [4700/6250]: 1.0144023895263672\n",
            "Loss [4800/6250]: 0.484499454498291\n",
            "Loss [4900/6250]: 1.2151943445205688\n",
            "Loss [5000/6250]: 1.2721593379974365\n",
            "Loss [5100/6250]: 0.874065637588501\n",
            "Loss [5200/6250]: 0.7590168714523315\n",
            "Loss [5300/6250]: 0.8781507015228271\n",
            "Loss [5400/6250]: 1.4611704349517822\n",
            "Loss [5500/6250]: 0.8381749987602234\n",
            "Loss [5600/6250]: 0.6934633255004883\n",
            "Loss [5700/6250]: 0.6962334513664246\n",
            "Loss [5800/6250]: 0.6744116544723511\n",
            "Loss [5900/6250]: 1.2653287649154663\n",
            "Loss [6000/6250]: 0.9590885043144226\n",
            "Loss [6100/6250]: 0.9853997230529785\n",
            "Loss [6200/6250]: 0.7513583302497864\n",
            "Train Accuracy: 71.698%\n",
            "Loss [0/1250]: 0.4662151634693146\n",
            "Loss [100/1250]: 0.7253601551055908\n",
            "Loss [200/1250]: 0.993003249168396\n",
            "Loss [300/1250]: 1.496460199356079\n",
            "Loss [400/1250]: 0.7230170369148254\n",
            "Loss [500/1250]: 1.3528292179107666\n",
            "Loss [600/1250]: 0.7456350326538086\n",
            "Loss [700/1250]: 0.8840821981430054\n",
            "Loss [800/1250]: 0.9521210789680481\n",
            "Loss [900/1250]: 0.9082464575767517\n",
            "Loss [1000/1250]: 0.70725417137146\n",
            "Loss [1100/1250]: 0.8721716403961182\n",
            "Loss [1200/1250]: 0.8371265530586243\n",
            "Test Accuracy: 72.54%\n",
            "epoch: 15\n",
            "Loss [0/6250]: 0.28222715854644775\n",
            "Loss [100/6250]: 1.2293121814727783\n",
            "Loss [200/6250]: 0.5769539475440979\n",
            "Loss [300/6250]: 0.45984241366386414\n",
            "Loss [400/6250]: 0.5809800624847412\n",
            "Loss [500/6250]: 0.9134178161621094\n",
            "Loss [600/6250]: 1.0136386156082153\n",
            "Loss [700/6250]: 1.0579512119293213\n",
            "Loss [800/6250]: 0.8330937027931213\n",
            "Loss [900/6250]: 0.37562236189842224\n",
            "Loss [1000/6250]: 0.665084183216095\n",
            "Loss [1100/6250]: 1.0369409322738647\n",
            "Loss [1200/6250]: 0.44160616397857666\n",
            "Loss [1300/6250]: 0.5135576725006104\n",
            "Loss [1400/6250]: 0.546434223651886\n",
            "Loss [1500/6250]: 0.48771950602531433\n",
            "Loss [1600/6250]: 0.7311015129089355\n",
            "Loss [1700/6250]: 0.9916634559631348\n",
            "Loss [1800/6250]: 0.5592072010040283\n",
            "Loss [1900/6250]: 2.335448741912842\n",
            "Loss [2000/6250]: 0.934115469455719\n",
            "Loss [2100/6250]: 0.5152225494384766\n",
            "Loss [2200/6250]: 2.02714204788208\n",
            "Loss [2300/6250]: 1.316689372062683\n",
            "Loss [2400/6250]: 0.7499464750289917\n",
            "Loss [2500/6250]: 0.6184262037277222\n",
            "Loss [2600/6250]: 0.8711081743240356\n",
            "Loss [2700/6250]: 0.514380931854248\n",
            "Loss [2800/6250]: 0.7529307007789612\n",
            "Loss [2900/6250]: 1.50475013256073\n",
            "Loss [3000/6250]: 0.9758543968200684\n",
            "Loss [3100/6250]: 0.5546479821205139\n",
            "Loss [3200/6250]: 0.8527433276176453\n",
            "Loss [3300/6250]: 1.2820357084274292\n",
            "Loss [3400/6250]: 0.46443289518356323\n",
            "Loss [3500/6250]: 0.3968505859375\n",
            "Loss [3600/6250]: 0.8602574467658997\n",
            "Loss [3700/6250]: 0.4116402268409729\n",
            "Loss [3800/6250]: 0.9718531966209412\n",
            "Loss [3900/6250]: 0.7115564942359924\n",
            "Loss [4000/6250]: 0.5333067774772644\n",
            "Loss [4100/6250]: 0.9999270439147949\n",
            "Loss [4200/6250]: 0.6486993432044983\n",
            "Loss [4300/6250]: 0.19460482895374298\n",
            "Loss [4400/6250]: 0.42616331577301025\n",
            "Loss [4500/6250]: 0.6925283074378967\n",
            "Loss [4600/6250]: 0.9268518686294556\n",
            "Loss [4700/6250]: 1.131630539894104\n",
            "Loss [4800/6250]: 0.6646355986595154\n",
            "Loss [4900/6250]: 1.260331630706787\n",
            "Loss [5000/6250]: 0.5011805891990662\n",
            "Loss [5100/6250]: 1.1468374729156494\n",
            "Loss [5200/6250]: 0.41320139169692993\n",
            "Loss [5300/6250]: 0.4304989278316498\n",
            "Loss [5400/6250]: 0.5877224206924438\n",
            "Loss [5500/6250]: 0.6207513213157654\n",
            "Loss [5600/6250]: 0.5911118388175964\n",
            "Loss [5700/6250]: 0.9513657093048096\n",
            "Loss [5800/6250]: 0.37380486726760864\n",
            "Loss [5900/6250]: 0.6750422120094299\n",
            "Loss [6000/6250]: 0.9517284035682678\n",
            "Loss [6100/6250]: 0.8921938538551331\n",
            "Loss [6200/6250]: 0.594783365726471\n",
            "Train Accuracy: 72.208%\n",
            "Loss [0/1250]: 0.26247313618659973\n",
            "Loss [100/1250]: 0.5875898003578186\n",
            "Loss [200/1250]: 1.0414953231811523\n",
            "Loss [300/1250]: 0.7337763905525208\n",
            "Loss [400/1250]: 1.1563053131103516\n",
            "Loss [500/1250]: 1.3669942617416382\n",
            "Loss [600/1250]: 0.8000410795211792\n",
            "Loss [700/1250]: 1.0043590068817139\n",
            "Loss [800/1250]: 0.8694734573364258\n",
            "Loss [900/1250]: 1.007857084274292\n",
            "Loss [1000/1250]: 0.6403856873512268\n",
            "Loss [1100/1250]: 1.0128499269485474\n",
            "Loss [1200/1250]: 0.9512332677841187\n",
            "Test Accuracy: 73.75%\n",
            "epoch: 16\n",
            "Loss [0/6250]: 0.9490923881530762\n",
            "Loss [100/6250]: 0.6678361296653748\n",
            "Loss [200/6250]: 1.0631272792816162\n",
            "Loss [300/6250]: 0.7493256330490112\n",
            "Loss [400/6250]: 0.3856133818626404\n",
            "Loss [500/6250]: 0.9690943360328674\n",
            "Loss [600/6250]: 0.57157301902771\n",
            "Loss [700/6250]: 0.5551608800888062\n",
            "Loss [800/6250]: 0.17687611281871796\n",
            "Loss [900/6250]: 1.0910289287567139\n",
            "Loss [1000/6250]: 0.857149600982666\n",
            "Loss [1100/6250]: 0.61590576171875\n",
            "Loss [1200/6250]: 0.49241575598716736\n",
            "Loss [1300/6250]: 0.8748377561569214\n",
            "Loss [1400/6250]: 0.5055256485939026\n",
            "Loss [1500/6250]: 0.9549943208694458\n",
            "Loss [1600/6250]: 0.4817400276660919\n",
            "Loss [1700/6250]: 0.7735412120819092\n",
            "Loss [1800/6250]: 0.9746732115745544\n",
            "Loss [1900/6250]: 1.2911561727523804\n",
            "Loss [2000/6250]: 0.3801382780075073\n",
            "Loss [2100/6250]: 1.0880184173583984\n",
            "Loss [2200/6250]: 1.040820598602295\n",
            "Loss [2300/6250]: 0.5036404132843018\n",
            "Loss [2400/6250]: 0.7366267442703247\n",
            "Loss [2500/6250]: 0.4222531318664551\n",
            "Loss [2600/6250]: 1.3103890419006348\n",
            "Loss [2700/6250]: 1.367432951927185\n",
            "Loss [2800/6250]: 0.25258299708366394\n",
            "Loss [2900/6250]: 0.6305407881736755\n",
            "Loss [3000/6250]: 0.13497653603553772\n",
            "Loss [3100/6250]: 0.7520267367362976\n",
            "Loss [3200/6250]: 0.5773015022277832\n",
            "Loss [3300/6250]: 1.2554484605789185\n",
            "Loss [3400/6250]: 0.5012308359146118\n",
            "Loss [3500/6250]: 0.5642249584197998\n",
            "Loss [3600/6250]: 0.5962357521057129\n",
            "Loss [3700/6250]: 0.5845577120780945\n",
            "Loss [3800/6250]: 0.6796457767486572\n",
            "Loss [3900/6250]: 0.9809425473213196\n",
            "Loss [4000/6250]: 1.1222965717315674\n",
            "Loss [4100/6250]: 0.7560087442398071\n",
            "Loss [4200/6250]: 1.1201938390731812\n",
            "Loss [4300/6250]: 0.7611221075057983\n",
            "Loss [4400/6250]: 0.6398705840110779\n",
            "Loss [4500/6250]: 0.8139187693595886\n",
            "Loss [4600/6250]: 1.0218676328659058\n",
            "Loss [4700/6250]: 0.6726318001747131\n",
            "Loss [4800/6250]: 0.5680758953094482\n",
            "Loss [4900/6250]: 0.2846236824989319\n",
            "Loss [5000/6250]: 1.8834497928619385\n",
            "Loss [5100/6250]: 0.5711890459060669\n",
            "Loss [5200/6250]: 1.0738502740859985\n",
            "Loss [5300/6250]: 0.5334051847457886\n",
            "Loss [5400/6250]: 0.32100367546081543\n",
            "Loss [5500/6250]: 0.9604387283325195\n",
            "Loss [5600/6250]: 1.47884202003479\n",
            "Loss [5700/6250]: 1.3681269884109497\n",
            "Loss [5800/6250]: 0.6872538328170776\n",
            "Loss [5900/6250]: 0.6513201594352722\n",
            "Loss [6000/6250]: 0.6546551585197449\n",
            "Loss [6100/6250]: 0.5663464665412903\n",
            "Loss [6200/6250]: 0.8033651113510132\n",
            "Train Accuracy: 72.61%\n",
            "Loss [0/1250]: 0.44546282291412354\n",
            "Loss [100/1250]: 0.8357243537902832\n",
            "Loss [200/1250]: 0.8350511193275452\n",
            "Loss [300/1250]: 0.7040112018585205\n",
            "Loss [400/1250]: 1.011150598526001\n",
            "Loss [500/1250]: 1.4900758266448975\n",
            "Loss [600/1250]: 0.4898534417152405\n",
            "Loss [700/1250]: 1.23893404006958\n",
            "Loss [800/1250]: 0.8771120309829712\n",
            "Loss [900/1250]: 0.9260873794555664\n",
            "Loss [1000/1250]: 0.7502654194831848\n",
            "Loss [1100/1250]: 1.1443233489990234\n",
            "Loss [1200/1250]: 1.179831862449646\n",
            "Test Accuracy: 73.25%\n",
            "epoch: 17\n",
            "Loss [0/6250]: 0.3997320234775543\n",
            "Loss [100/6250]: 0.7581612467765808\n",
            "Loss [200/6250]: 0.727123498916626\n",
            "Loss [300/6250]: 0.4310995042324066\n",
            "Loss [400/6250]: 0.7773429751396179\n",
            "Loss [500/6250]: 1.0138945579528809\n",
            "Loss [600/6250]: 2.3272721767425537\n",
            "Loss [700/6250]: 0.688348650932312\n",
            "Loss [800/6250]: 0.5248634815216064\n",
            "Loss [900/6250]: 0.3096734583377838\n",
            "Loss [1000/6250]: 0.10074307024478912\n",
            "Loss [1100/6250]: 0.7183474898338318\n",
            "Loss [1200/6250]: 1.5971328020095825\n",
            "Loss [1300/6250]: 1.0053446292877197\n",
            "Loss [1400/6250]: 1.0964155197143555\n",
            "Loss [1500/6250]: 0.6047574877738953\n",
            "Loss [1600/6250]: 0.6904577016830444\n",
            "Loss [1700/6250]: 0.5625914335250854\n",
            "Loss [1800/6250]: 1.3344732522964478\n",
            "Loss [1900/6250]: 0.7910690307617188\n",
            "Loss [2000/6250]: 0.9953151345252991\n",
            "Loss [2100/6250]: 0.4387674927711487\n",
            "Loss [2200/6250]: 0.8116650581359863\n",
            "Loss [2300/6250]: 1.6328306198120117\n",
            "Loss [2400/6250]: 0.9740157127380371\n",
            "Loss [2500/6250]: 0.9811426997184753\n",
            "Loss [2600/6250]: 0.3862970471382141\n",
            "Loss [2700/6250]: 0.5630789995193481\n",
            "Loss [2800/6250]: 0.4053075611591339\n",
            "Loss [2900/6250]: 0.7450404763221741\n",
            "Loss [3000/6250]: 0.46221739053726196\n",
            "Loss [3100/6250]: 1.333724021911621\n",
            "Loss [3200/6250]: 0.9019907116889954\n",
            "Loss [3300/6250]: 0.13429491221904755\n",
            "Loss [3400/6250]: 1.1662349700927734\n",
            "Loss [3500/6250]: 0.8545846343040466\n",
            "Loss [3600/6250]: 0.865186333656311\n",
            "Loss [3700/6250]: 0.9224126935005188\n",
            "Loss [3800/6250]: 0.3006869852542877\n",
            "Loss [3900/6250]: 1.2927594184875488\n",
            "Loss [4000/6250]: 1.1501638889312744\n",
            "Loss [4100/6250]: 0.29326051473617554\n",
            "Loss [4200/6250]: 0.5766582489013672\n",
            "Loss [4300/6250]: 1.0646504163742065\n",
            "Loss [4400/6250]: 0.7301605939865112\n",
            "Loss [4500/6250]: 1.1711233854293823\n",
            "Loss [4600/6250]: 1.368567943572998\n",
            "Loss [4700/6250]: 0.9499422907829285\n",
            "Loss [4800/6250]: 0.5424060821533203\n",
            "Loss [4900/6250]: 0.4877397418022156\n",
            "Loss [5000/6250]: 0.9965212345123291\n",
            "Loss [5100/6250]: 1.0409095287322998\n",
            "Loss [5200/6250]: 0.7711302042007446\n",
            "Loss [5300/6250]: 0.6824865937232971\n",
            "Loss [5400/6250]: 1.7575604915618896\n",
            "Loss [5500/6250]: 0.7897510528564453\n",
            "Loss [5600/6250]: 0.43280676007270813\n",
            "Loss [5700/6250]: 0.4084545969963074\n",
            "Loss [5800/6250]: 0.4292224943637848\n",
            "Loss [5900/6250]: 0.9073371887207031\n",
            "Loss [6000/6250]: 0.7270275354385376\n",
            "Loss [6100/6250]: 1.1902776956558228\n",
            "Loss [6200/6250]: 0.9385024309158325\n",
            "Train Accuracy: 72.882%\n",
            "Loss [0/1250]: 0.3446875810623169\n",
            "Loss [100/1250]: 0.7043491005897522\n",
            "Loss [200/1250]: 1.0721235275268555\n",
            "Loss [300/1250]: 1.247051477432251\n",
            "Loss [400/1250]: 1.0906800031661987\n",
            "Loss [500/1250]: 1.470869541168213\n",
            "Loss [600/1250]: 0.5991358160972595\n",
            "Loss [700/1250]: 1.2828574180603027\n",
            "Loss [800/1250]: 0.7634432315826416\n",
            "Loss [900/1250]: 0.684323251247406\n",
            "Loss [1000/1250]: 0.7644357085227966\n",
            "Loss [1100/1250]: 0.799258828163147\n",
            "Loss [1200/1250]: 0.8399036526679993\n",
            "Test Accuracy: 73.45%\n",
            "epoch: 18\n",
            "Loss [0/6250]: 0.7287273406982422\n",
            "Loss [100/6250]: 0.5679884552955627\n",
            "Loss [200/6250]: 0.9166942834854126\n",
            "Loss [300/6250]: 0.5454981327056885\n",
            "Loss [400/6250]: 0.750400185585022\n",
            "Loss [500/6250]: 0.5749012231826782\n",
            "Loss [600/6250]: 0.8148364424705505\n",
            "Loss [700/6250]: 0.9870084524154663\n",
            "Loss [800/6250]: 0.533438503742218\n",
            "Loss [900/6250]: 0.8001773953437805\n",
            "Loss [1000/6250]: 0.5864921808242798\n",
            "Loss [1100/6250]: 0.41492533683776855\n",
            "Loss [1200/6250]: 0.9797499179840088\n",
            "Loss [1300/6250]: 0.4280579090118408\n",
            "Loss [1400/6250]: 0.8288166522979736\n",
            "Loss [1500/6250]: 0.7489789724349976\n",
            "Loss [1600/6250]: 0.6962605714797974\n",
            "Loss [1700/6250]: 0.9163776636123657\n",
            "Loss [1800/6250]: 1.5905680656433105\n",
            "Loss [1900/6250]: 0.7394957542419434\n",
            "Loss [2000/6250]: 0.886137843132019\n",
            "Loss [2100/6250]: 0.37931618094444275\n",
            "Loss [2200/6250]: 0.6340739727020264\n",
            "Loss [2300/6250]: 0.7678872346878052\n",
            "Loss [2400/6250]: 0.5740247368812561\n",
            "Loss [2500/6250]: 0.7405293583869934\n",
            "Loss [2600/6250]: 1.1505087614059448\n",
            "Loss [2700/6250]: 1.0090186595916748\n",
            "Loss [2800/6250]: 1.6428245306015015\n",
            "Loss [2900/6250]: 0.6102371215820312\n",
            "Loss [3000/6250]: 1.0111844539642334\n",
            "Loss [3100/6250]: 0.7259019613265991\n",
            "Loss [3200/6250]: 0.4520453214645386\n",
            "Loss [3300/6250]: 0.9050526022911072\n",
            "Loss [3400/6250]: 0.2374480962753296\n",
            "Loss [3500/6250]: 1.0651769638061523\n",
            "Loss [3600/6250]: 0.7407429814338684\n",
            "Loss [3700/6250]: 1.412580966949463\n",
            "Loss [3800/6250]: 1.7368074655532837\n",
            "Loss [3900/6250]: 0.954275906085968\n",
            "Loss [4000/6250]: 0.6955997347831726\n",
            "Loss [4100/6250]: 0.22486965358257294\n",
            "Loss [4200/6250]: 0.46437519788742065\n",
            "Loss [4300/6250]: 0.33649930357933044\n",
            "Loss [4400/6250]: 0.41913068294525146\n",
            "Loss [4500/6250]: 0.8606379628181458\n",
            "Loss [4600/6250]: 1.0663254261016846\n",
            "Loss [4700/6250]: 0.6652454733848572\n",
            "Loss [4800/6250]: 0.5081409215927124\n",
            "Loss [4900/6250]: 0.13287341594696045\n",
            "Loss [5000/6250]: 0.519877552986145\n",
            "Loss [5100/6250]: 0.6781687140464783\n",
            "Loss [5200/6250]: 0.30236831307411194\n",
            "Loss [5300/6250]: 0.6868878602981567\n",
            "Loss [5400/6250]: 0.6472641825675964\n",
            "Loss [5500/6250]: 0.9971933960914612\n",
            "Loss [5600/6250]: 0.7632750868797302\n",
            "Loss [5700/6250]: 0.42571842670440674\n",
            "Loss [5800/6250]: 0.7559375762939453\n",
            "Loss [5900/6250]: 0.3636794090270996\n",
            "Loss [6000/6250]: 0.3797132074832916\n",
            "Loss [6100/6250]: 0.700724184513092\n",
            "Loss [6200/6250]: 0.5046948194503784\n",
            "Train Accuracy: 73.21600000000001%\n",
            "Loss [0/1250]: 0.32258304953575134\n",
            "Loss [100/1250]: 0.9201179146766663\n",
            "Loss [200/1250]: 0.7984450459480286\n",
            "Loss [300/1250]: 0.7192550897598267\n",
            "Loss [400/1250]: 1.0363394021987915\n",
            "Loss [500/1250]: 1.3143738508224487\n",
            "Loss [600/1250]: 0.3843953609466553\n",
            "Loss [700/1250]: 1.215972661972046\n",
            "Loss [800/1250]: 1.0398383140563965\n",
            "Loss [900/1250]: 0.6319230794906616\n",
            "Loss [1000/1250]: 0.33538585901260376\n",
            "Loss [1100/1250]: 1.2166866064071655\n",
            "Loss [1200/1250]: 0.9514987468719482\n",
            "Test Accuracy: 74.28%\n",
            "epoch: 19\n",
            "Loss [0/6250]: 1.1714791059494019\n",
            "Loss [100/6250]: 1.0153754949569702\n",
            "Loss [200/6250]: 0.741969883441925\n",
            "Loss [300/6250]: 1.078050136566162\n",
            "Loss [400/6250]: 1.3008970022201538\n",
            "Loss [500/6250]: 0.6980674862861633\n",
            "Loss [600/6250]: 1.0235527753829956\n",
            "Loss [700/6250]: 0.22765949368476868\n",
            "Loss [800/6250]: 0.5943583846092224\n",
            "Loss [900/6250]: 1.0123902559280396\n",
            "Loss [1000/6250]: 1.4970316886901855\n",
            "Loss [1100/6250]: 0.9883440732955933\n",
            "Loss [1200/6250]: 1.3045823574066162\n",
            "Loss [1300/6250]: 0.8339403867721558\n",
            "Loss [1400/6250]: 0.2819162607192993\n",
            "Loss [1500/6250]: 1.4906768798828125\n",
            "Loss [1600/6250]: 0.3007112145423889\n",
            "Loss [1700/6250]: 0.9685925245285034\n",
            "Loss [1800/6250]: 0.3865867853164673\n",
            "Loss [1900/6250]: 0.6007792353630066\n",
            "Loss [2000/6250]: 1.0844390392303467\n",
            "Loss [2100/6250]: 0.4807894229888916\n",
            "Loss [2200/6250]: 0.5279216170310974\n",
            "Loss [2300/6250]: 0.6519608497619629\n",
            "Loss [2400/6250]: 0.5453974008560181\n",
            "Loss [2500/6250]: 0.5722806453704834\n",
            "Loss [2600/6250]: 0.5815623998641968\n",
            "Loss [2700/6250]: 0.4294229745864868\n",
            "Loss [2800/6250]: 0.9068670272827148\n",
            "Loss [2900/6250]: 0.9227843880653381\n",
            "Loss [3000/6250]: 0.7694591283798218\n",
            "Loss [3100/6250]: 0.1914091557264328\n",
            "Loss [3200/6250]: 0.5968855619430542\n",
            "Loss [3300/6250]: 0.56119304895401\n",
            "Loss [3400/6250]: 0.3824339807033539\n",
            "Loss [3500/6250]: 0.5562143325805664\n",
            "Loss [3600/6250]: 0.580461859703064\n",
            "Loss [3700/6250]: 1.1018420457839966\n",
            "Loss [3800/6250]: 1.7158112525939941\n",
            "Loss [3900/6250]: 0.6512610912322998\n",
            "Loss [4000/6250]: 0.3672414720058441\n",
            "Loss [4100/6250]: 0.3691156506538391\n",
            "Loss [4200/6250]: 0.6606128811836243\n",
            "Loss [4300/6250]: 0.44947296380996704\n",
            "Loss [4400/6250]: 0.7028416991233826\n",
            "Loss [4500/6250]: 1.0454238653182983\n",
            "Loss [4600/6250]: 0.3845518231391907\n",
            "Loss [4700/6250]: 0.414387047290802\n",
            "Loss [4800/6250]: 1.0495282411575317\n",
            "Loss [4900/6250]: 0.8695451021194458\n",
            "Loss [5000/6250]: 0.32695162296295166\n",
            "Loss [5100/6250]: 0.5272724628448486\n",
            "Loss [5200/6250]: 0.6001675724983215\n",
            "Loss [5300/6250]: 1.064625859260559\n",
            "Loss [5400/6250]: 0.599455714225769\n",
            "Loss [5500/6250]: 0.9137744903564453\n",
            "Loss [5600/6250]: 0.9398839473724365\n",
            "Loss [5700/6250]: 0.8652689456939697\n",
            "Loss [5800/6250]: 0.6515785455703735\n",
            "Loss [5900/6250]: 0.687549352645874\n",
            "Loss [6000/6250]: 0.9783364534378052\n",
            "Loss [6100/6250]: 1.1043603420257568\n",
            "Loss [6200/6250]: 1.3675272464752197\n",
            "Train Accuracy: 73.516%\n",
            "Loss [0/1250]: 0.38724225759506226\n",
            "Loss [100/1250]: 0.6563971638679504\n",
            "Loss [200/1250]: 0.9306402206420898\n",
            "Loss [300/1250]: 1.0495446920394897\n",
            "Loss [400/1250]: 0.649375319480896\n",
            "Loss [500/1250]: 1.146345615386963\n",
            "Loss [600/1250]: 0.5978390574455261\n",
            "Loss [700/1250]: 1.0406147241592407\n",
            "Loss [800/1250]: 0.8098083734512329\n",
            "Loss [900/1250]: 0.9595944881439209\n",
            "Loss [1000/1250]: 0.6617844700813293\n",
            "Loss [1100/1250]: 1.070353627204895\n",
            "Loss [1200/1250]: 1.4665334224700928\n",
            "Test Accuracy: 74.68%\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(20):\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    train_epoch(train_dataloader)\n",
        "    test_epoch(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7ow1h231gE"
      },
      "source": [
        "Is your model working well on CIFAR-10 dataset?\n",
        "\n",
        "Try to use a different optimizer, learning rate scheduler, data augmentation, etc. to improve the performance of your model.\n",
        "Alternatively, you can try to use a different model (e.g. ResNet-34, ResNet-50, etc.) to achieve better performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}